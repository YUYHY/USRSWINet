22-05-10 21:07:08.819 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 10000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:07:11.347 : Number of train images: 3,450, iters: 1,150
22-05-10 21:08:48.756 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 10000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:08:49.600 : Number of train images: 3,450, iters: 1,150
22-05-10 21:15:38.627 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 10000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:15:39.586 : Number of train images: 3,450, iters: 1,150
22-05-10 21:33:54.876 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 10000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:33:55.778 : Number of train images: 3,450, iters: 1,150
22-05-10 21:41:20.713 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 10000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:41:21.705 : Number of train images: 3,450, iters: 1,150
22-05-10 21:41:24.707 : 
Networks name: USRNet_ST
Params number: 3624844
Net structure:
USRNet_ST(
  (d): DataNet()
  (p): ResUNet(
    (m_head): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (m_down1): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down2): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down3): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (sw_res1): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (2): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (3): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res2): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (2): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (3): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res3): SwinIR(
      (conv_first): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (2): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (3): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res4): SwinIR(
      (conv_first): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (2): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (3): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (m_up3): Sequential(
      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up2): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up1): Sequential(
      (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_tail): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (h): HyPaNet(
    (mlp): Sequential(
      (0): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (3): ReLU(inplace=True)
      (4): Conv2d(32, 12, kernel_size=(1, 1), stride=(1, 1))
      (5): Softplus(beta=1, threshold=20)
    )
  )
)

22-05-10 21:41:24.941 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.002 | -0.084 |  0.102 |  0.033 | torch.Size([16, 4, 3, 3]) || p.m_head.weight
 | -0.000 | -0.054 |  0.048 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.0.weight
 | -0.000 | -0.056 |  0.054 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.2.weight
 |  0.000 | -0.058 |  0.059 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.0.weight
 |  0.000 | -0.060 |  0.052 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.2.weight
 |  0.000 | -0.071 |  0.089 |  0.025 | torch.Size([32, 16, 2, 2]) || p.m_down1.2.weight
 | -0.000 | -0.042 |  0.046 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.0.weight
 |  0.000 | -0.048 |  0.049 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.2.weight
 |  0.000 | -0.045 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.0.weight
 | -0.000 | -0.044 |  0.042 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.2.weight
 | -0.000 | -0.069 |  0.076 |  0.018 | torch.Size([64, 32, 2, 2]) || p.m_down2.2.weight
 | -0.000 | -0.038 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.0.weight
 | -0.000 | -0.036 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.2.weight
 | -0.000 | -0.036 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.0.weight
 |  0.000 | -0.030 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.2.weight
 | -0.000 | -0.052 |  0.050 |  0.013 | torch.Size([64, 64, 2, 2]) || p.m_down3.2.weight
 | -0.000 | -0.034 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.bias
 | -0.002 | -0.053 |  0.038 |  0.018 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.054 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.087 |  0.088 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.058 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.065 |  0.070 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.046 |  0.040 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.062 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.085 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.065 |  0.064 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.075 |  0.072 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.058 |  0.070 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.055 |  0.056 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.080 |  0.090 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.062 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.061 |  0.064 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.045 |  0.042 |  0.018 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.055 |  0.051 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.084 |  0.077 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.073 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.067 |  0.063 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.056 |  0.048 |  0.022 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.057 |  0.051 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.085 |  0.095 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.058 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.079 |  0.067 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.055 |  0.048 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.057 |  0.058 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.088 |  0.086 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.063 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.060 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.033 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.bias
 |  0.002 | -0.044 |  0.055 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.053 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.092 |  0.079 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.062 |  0.062 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.063 |  0.060 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.bias
 |  0.002 | -0.046 |  0.061 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.053 |  0.049 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.086 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.065 |  0.063 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.061 |  0.062 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.048 |  0.062 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.053 |  0.050 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.096 |  0.084 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.063 |  0.068 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.066 |  0.060 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.052 |  0.040 |  0.018 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.054 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.084 |  0.087 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.059 |  0.072 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.062 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.norm1.bias
 |  0.002 | -0.042 |  0.072 |  0.024 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.053 |  0.054 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.qkv.bias
 |  0.001 | -0.100 |  0.097 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.068 |  0.072 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.065 |  0.066 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.norm1.bias
 | -0.002 | -0.053 |  0.052 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.053 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.096 |  0.099 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.058 |  0.074 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.067 |  0.062 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.034 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.norm1.bias
 |  0.003 | -0.047 |  0.055 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.050 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.089 |  0.080 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.062 |  0.075 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.063 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.041 |  0.043 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.055 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.095 |  0.085 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.064 |  0.063 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.067 |  0.061 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.norm1.bias
 | -0.004 | -0.048 |  0.050 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.051 |  0.054 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.085 |  0.079 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.065 |  0.072 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.063 |  0.069 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.046 |  0.050 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.057 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.090 |  0.117 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.070 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.060 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.057 |  0.053 |  0.022 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.057 |  0.058 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.001 | -0.091 |  0.077 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.065 |  0.070 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.066 |  0.064 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.067 |  0.047 |  0.022 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.054 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.082 |  0.089 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.063 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.061 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.036 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.2.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.069 |  0.050 |  0.022 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.053 |  0.049 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.092 |  0.089 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.060 |  0.057 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.063 |  0.060 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.050 |  0.034 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.053 |  0.063 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.089 |  0.074 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.069 |  0.064 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.071 |  0.074 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.norm1.bias
 |  0.004 | -0.042 |  0.069 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.057 |  0.058 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.100 |  0.086 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.063 |  0.062 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.061 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.037 |  0.044 |  0.016 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.058 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.083 |  0.092 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.068 |  0.058 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.072 |  0.064 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.043 |  0.046 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.061 |  0.062 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.093 |  0.072 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.061 |  0.071 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.081 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.039 |  0.051 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.052 |  0.062 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.090 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.069 |  0.067 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.071 |  0.058 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.033 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.3.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.norm.bias
 |  0.000 | -0.032 |  0.030 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_after_body.bias
 | -0.000 | -0.033 |  0.031 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_before_upsample.0.bias
 |  0.000 | -0.044 |  0.035 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res1.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res1.upsample.0.bias
 | -0.000 | -0.037 |  0.038 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_last.bias
 |  0.000 | -0.034 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.048 |  0.063 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.057 |  0.059 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.081 |  0.098 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.066 |  0.057 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.065 |  0.063 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.bias
 | -0.002 | -0.045 |  0.044 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.052 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.083 |  0.096 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.062 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.066 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.048 |  0.051 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.054 |  0.049 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.093 |  0.081 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.058 |  0.065 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.063 |  0.066 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.062 |  0.055 |  0.022 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.055 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.077 |  0.087 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.064 |  0.071 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.064 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.norm1.bias
 |  0.002 | -0.044 |  0.046 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.068 |  0.058 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.082 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.070 |  0.062 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.064 |  0.058 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.norm1.bias
 |  0.003 | -0.047 |  0.038 |  0.021 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.055 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.090 |  0.090 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.061 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.061 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.037 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.040 |  0.052 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.054 |  0.053 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.092 |  0.090 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.061 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.072 |  0.064 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.064 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.052 |  0.051 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.088 |  0.100 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.065 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.069 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.045 |  0.040 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.053 |  0.050 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.077 |  0.097 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.062 |  0.071 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.063 |  0.073 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.bias
 | -0.004 | -0.051 |  0.055 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.056 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.105 |  0.083 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.065 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.074 |  0.066 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.norm1.bias
 |  0.002 | -0.063 |  0.057 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.056 |  0.053 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.101 |  0.094 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.062 |  0.067 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.065 |  0.064 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.057 |  0.044 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.052 |  0.061 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.096 |  0.085 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.059 |  0.064 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.062 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.034 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.051 |  0.070 |  0.023 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.054 |  0.062 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.087 |  0.106 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.061 |  0.072 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.067 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.norm1.bias
 | -0.006 | -0.055 |  0.057 |  0.025 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.053 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.089 |  0.097 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.063 |  0.063 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.069 |  0.073 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.051 |  0.048 |  0.021 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.052 |  0.056 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.082 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.070 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.072 |  0.074 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.045 |  0.058 |  0.021 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.054 |  0.056 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.090 |  0.089 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.070 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.066 |  0.060 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.norm1.bias
 | -0.002 | -0.044 |  0.041 |  0.017 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.051 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.090 |  0.079 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.068 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.067 |  0.066 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.045 |  0.051 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.054 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.qkv.bias
 |  0.001 | -0.083 |  0.089 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.068 |  0.064 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.068 |  0.062 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.038 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.2.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.057 |  0.044 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.053 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.078 |  0.097 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.059 |  0.063 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.063 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.norm1.bias
 | -0.005 | -0.051 |  0.042 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.059 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.074 |  0.084 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.063 |  0.069 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.061 |  0.061 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.043 |  0.050 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.055 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.079 |  0.089 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.075 |  0.057 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.064 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.067 |  0.056 |  0.022 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.057 |  0.059 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.100 |  0.085 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.061 |  0.075 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.066 |  0.063 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.norm1.bias
 |  0.003 | -0.039 |  0.049 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.059 |  0.054 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.086 |  0.095 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.061 |  0.068 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.066 |  0.070 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.061 |  0.045 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.056 |  0.059 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.096 |  0.085 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.065 |  0.070 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.062 |  0.079 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.034 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.3.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.norm.bias
 | -0.000 | -0.033 |  0.031 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_after_body.bias
 |  0.000 | -0.032 |  0.031 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_before_upsample.0.bias
 | -0.000 | -0.039 |  0.036 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res2.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res2.upsample.0.bias
 | -0.000 | -0.033 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_last.bias
 | -0.000 | -0.045 |  0.041 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.bias
 |  0.002 | -0.043 |  0.043 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.001 | -0.063 |  0.077 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.120 |  0.094 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.077 |  0.077 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.077 |  0.078 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.bias
 |  0.002 | -0.043 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.064 |  0.077 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.123 |  0.112 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.077 |  0.082 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.085 |  0.080 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.bias
 |  0.004 | -0.043 |  0.044 |  0.018 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.071 |  0.073 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.001 | -0.110 |  0.102 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.bias
 | -0.001 | -0.082 |  0.084 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.089 |  0.091 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.044 |  0.057 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.081 |  0.064 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.113 |  0.109 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.084 |  0.092 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.075 |  0.082 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.049 |  0.042 |  0.019 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.059 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.qkv.bias
 |  0.001 | -0.099 |  0.109 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.norm2.bias
 | -0.001 | -0.087 |  0.073 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.076 |  0.086 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.norm1.bias
 | -0.003 | -0.060 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.067 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.112 |  0.092 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.norm2.bias
 | -0.001 | -0.074 |  0.098 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.5.mlp.fc1.bias
 |  0.001 | -0.092 |  0.082 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.048 |  0.048 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.bias
 | -0.002 | -0.060 |  0.040 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.077 |  0.076 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.001 | -0.098 |  0.103 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.072 |  0.087 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.073 |  0.084 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.047 |  0.057 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.075 |  0.075 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.001 | -0.108 |  0.114 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.085 |  0.105 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.078 |  0.078 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.bias
 |  0.003 | -0.047 |  0.063 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.066 |  0.066 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.002 | -0.101 |  0.110 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.bias
 | -0.001 | -0.075 |  0.075 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.001 | -0.079 |  0.073 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.057 |  0.055 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.082 |  0.065 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.108 |  0.101 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.074 |  0.090 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.084 |  0.087 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.049 |  0.062 |  0.022 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.001 | -0.075 |  0.074 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.qkv.bias
 |  0.001 | -0.095 |  0.107 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.norm2.bias
 |  0.001 | -0.086 |  0.085 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.087 |  0.083 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.norm1.bias
 | -0.002 | -0.053 |  0.056 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.065 |  0.075 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.qkv.bias
 |  0.001 | -0.120 |  0.105 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.norm2.bias
 |  0.001 | -0.079 |  0.074 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.090 |  0.089 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.042 |  0.051 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.049 |  0.056 |  0.018 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.069 |  0.079 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.003 | -0.101 |  0.100 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.093 |  0.077 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.100 |  0.083 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.norm1.bias
 |  0.002 | -0.042 |  0.049 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.084 |  0.071 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.106 |  0.126 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.080 |  0.082 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.083 |  0.080 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.norm1.bias
 |  0.002 | -0.043 |  0.037 |  0.017 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.066 |  0.068 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.104 |  0.124 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.norm2.bias
 | -0.001 | -0.097 |  0.079 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.079 |  0.080 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.norm1.bias
 |  0.003 | -0.053 |  0.062 |  0.023 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.065 |  0.066 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.107 |  0.099 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.097 |  0.083 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.072 |  0.083 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.norm1.bias
 | -0.002 | -0.051 |  0.049 |  0.018 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.068 |  0.072 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.107 |  0.109 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.083 |  0.081 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.079 |  0.095 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.norm1.bias
 |  0.002 | -0.055 |  0.049 |  0.022 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.001 | -0.083 |  0.062 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.qkv.bias
 |  0.001 | -0.118 |  0.102 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.norm2.bias
 |  0.001 | -0.086 |  0.087 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.081 |  0.085 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.039 |  0.050 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.2.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.norm1.bias
 | -0.002 | -0.053 |  0.042 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.084 |  0.068 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.001 | -0.110 |  0.114 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.086 |  0.086 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.083 |  0.084 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.norm1.bias
 | -0.002 | -0.062 |  0.048 |  0.022 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.061 |  0.066 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.114 |  0.128 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.norm2.bias
 | -0.001 | -0.090 |  0.072 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.078 |  0.075 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.048 |  0.058 |  0.022 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.071 |  0.066 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.114 |  0.117 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.080 |  0.069 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.081 |  0.074 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.norm1.bias
 | -0.003 | -0.058 |  0.043 |  0.018 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.relative_position_index
 |  0.001 | -0.071 |  0.080 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.001 | -0.113 |  0.114 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.073 |  0.086 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.094 |  0.071 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.062 |  0.054 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.072 |  0.080 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.112 |  0.115 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.norm2.bias
 | -0.001 | -0.084 |  0.084 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.001 | -0.083 |  0.097 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.064 |  0.043 |  0.018 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.070 |  0.065 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.001 | -0.098 |  0.099 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.079 |  0.080 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.078 |  0.080 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.042 |  0.043 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.3.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.norm.bias
 |  0.000 | -0.046 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_after_body.bias
 | -0.000 | -0.044 |  0.044 |  0.012 | torch.Size([64, 32, 3, 3]) || p.sw_res3.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.conv_before_upsample.0.bias
 | -0.000 | -0.040 |  0.035 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res3.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res3.upsample.0.bias
 |  0.000 | -0.033 |  0.036 |  0.008 | torch.Size([32, 64, 3, 3]) || p.sw_res3.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_last.bias
 | -0.000 | -0.062 |  0.061 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.043 |  0.047 |  0.016 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.001 | -0.080 |  0.079 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.004 | -0.122 |  0.132 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.bias
 | -0.001 | -0.104 |  0.106 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.126 |  0.101 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.037 |  0.039 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.074 |  0.076 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.002 | -0.111 |  0.125 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.bias
 | -0.002 | -0.115 |  0.097 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.003 | -0.082 |  0.100 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.bias
 |  0.005 | -0.050 |  0.048 |  0.018 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.001 | -0.081 |  0.072 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.124 |  0.170 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.bias
 | -0.003 | -0.104 |  0.089 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.002 | -0.104 |  0.103 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.bias
 | -0.002 | -0.043 |  0.051 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.085 |  0.097 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.005 | -0.118 |  0.120 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.bias
 | -0.002 | -0.098 |  0.100 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.102 |  0.106 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.norm1.bias
 | -0.003 | -0.058 |  0.055 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.002 | -0.083 |  0.097 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.001 | -0.150 |  0.145 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.norm2.bias
 |  0.002 | -0.100 |  0.094 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.002 | -0.111 |  0.100 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.norm1.bias
 |  0.004 | -0.040 |  0.058 |  0.022 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.094 |  0.110 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.116 |  0.126 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.norm2.bias
 | -0.001 | -0.116 |  0.091 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.096 |  0.102 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.052 |  0.056 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.052 |  0.049 |  0.022 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.003 | -0.099 |  0.086 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.004 | -0.130 |  0.132 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.104 |  0.092 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.003 | -0.103 |  0.105 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.047 |  0.043 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.001 | -0.082 |  0.078 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.004 | -0.145 |  0.134 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.087 |  0.101 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.001 | -0.108 |  0.094 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.049 |  0.069 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.068 |  0.082 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.130 |  0.108 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.bias
 |  0.001 | -0.115 |  0.100 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.002 | -0.103 |  0.094 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.044 |  0.042 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.001 | -0.092 |  0.088 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.003 | -0.124 |  0.120 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.123 |  0.089 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.124 |  0.101 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.048 |  0.046 |  0.018 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.001 | -0.081 |  0.087 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.002 | -0.109 |  0.151 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.norm2.bias
 | -0.003 | -0.114 |  0.106 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.4.mlp.fc1.bias
 | -0.001 | -0.098 |  0.107 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.norm1.bias
 |  0.002 | -0.045 |  0.057 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.002 | -0.100 |  0.078 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.005 | -0.115 |  0.137 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.norm2.bias
 | -0.001 | -0.114 |  0.097 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.5.mlp.fc1.bias
 | -0.001 | -0.117 |  0.087 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.054 |  0.065 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.045 |  0.035 |  0.017 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.084 |  0.080 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.123 |  0.118 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.norm2.bias
 |  0.004 | -0.106 |  0.095 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.113 |  0.092 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.057 |  0.064 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.092 |  0.094 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.154 |  0.128 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.106 |  0.102 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.002 | -0.102 |  0.089 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.043 |  0.036 |  0.017 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.095 |  0.097 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.110 |  0.123 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.norm2.bias
 | -0.002 | -0.122 |  0.126 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.001 | -0.102 |  0.084 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.045 |  0.055 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.092 |  0.091 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.002 | -0.139 |  0.124 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.091 |  0.089 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.004 | -0.091 |  0.106 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.norm1.bias
 | -0.003 | -0.045 |  0.043 |  0.018 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.082 |  0.087 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.001 | -0.158 |  0.158 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.norm2.bias
 | -0.003 | -0.097 |  0.103 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.002 | -0.095 |  0.092 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.046 |  0.059 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.001 | -0.080 |  0.089 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.003 | -0.130 |  0.125 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.111 |  0.129 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.104 |  0.110 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.001 | -0.058 |  0.059 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.2.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.057 |  0.059 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.002 | -0.082 |  0.091 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.002 | -0.143 |  0.131 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.105 |  0.102 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.0.mlp.fc1.bias
 | -0.001 | -0.100 |  0.101 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.039 |  0.039 |  0.016 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.relative_position_index
 |  0.001 | -0.076 |  0.101 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.005 | -0.130 |  0.133 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.087 |  0.093 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.107 |  0.116 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.norm1.bias
 |  0.003 | -0.042 |  0.053 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.relative_position_index
 | -0.002 | -0.094 |  0.079 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.002 | -0.116 |  0.123 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.norm2.bias
 |  0.002 | -0.109 |  0.088 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.001 | -0.087 |  0.092 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.norm1.bias
 |  0.003 | -0.052 |  0.058 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.075 |  0.085 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.002 | -0.130 |  0.121 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.107 |  0.095 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.093 |  0.091 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.056 |  0.049 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.001 | -0.087 |  0.091 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.110 |  0.129 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.norm2.bias
 |  0.001 | -0.103 |  0.115 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.110 |  0.100 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.048 |  0.056 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.086 |  0.083 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.004 | -0.131 |  0.137 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.114 |  0.105 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.002 | -0.085 |  0.099 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.055 |  0.061 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.3.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.norm.bias
 |  0.001 | -0.059 |  0.056 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_after_body.bias
 | -0.000 | -0.069 |  0.069 |  0.017 | torch.Size([64, 16, 3, 3]) || p.sw_res4.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res4.conv_before_upsample.0.bias
 | -0.000 | -0.036 |  0.038 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res4.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res4.upsample.0.bias
 | -0.000 | -0.032 |  0.032 |  0.008 | torch.Size([16, 64, 3, 3]) || p.sw_res4.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_last.bias
 | -0.000 | -0.047 |  0.053 |  0.013 | torch.Size([64, 64, 2, 2]) || p.m_up3.0.weight
 | -0.000 | -0.032 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.0.weight
 |  0.000 | -0.035 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.2.weight
 |  0.000 | -0.035 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.0.weight
 | -0.000 | -0.033 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.2.weight
 |  0.000 | -0.064 |  0.062 |  0.018 | torch.Size([64, 32, 2, 2]) || p.m_up2.0.weight
 |  0.000 | -0.050 |  0.044 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.0.weight
 | -0.000 | -0.043 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.2.weight
 |  0.000 | -0.041 |  0.047 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.0.weight
 |  0.000 | -0.045 |  0.044 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.2.weight
 | -0.000 | -0.084 |  0.079 |  0.025 | torch.Size([32, 16, 2, 2]) || p.m_up1.0.weight
 | -0.000 | -0.059 |  0.058 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.0.weight
 |  0.000 | -0.059 |  0.055 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.2.weight
 |  0.000 | -0.050 |  0.061 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.0.weight
 |  0.000 | -0.058 |  0.064 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.2.weight
 |  0.000 | -0.055 |  0.053 |  0.017 | torch.Size([3, 16, 3, 3]) || p.m_tail.weight
 |  0.006 | -0.078 |  0.070 |  0.035 | torch.Size([32, 2, 1, 1]) || h.mlp.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || h.mlp.0.bias
 |  0.000 | -0.106 |  0.094 |  0.035 | torch.Size([32, 32, 1, 1]) || h.mlp.2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || h.mlp.2.bias
 | -0.001 | -0.096 |  0.115 |  0.035 | torch.Size([12, 32, 1, 1]) || h.mlp.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([12]) || h.mlp.4.bias

