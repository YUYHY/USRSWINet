22-05-10 21:07:08.819 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 10000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:07:11.347 : Number of train images: 3,450, iters: 1,150
22-05-10 21:08:48.756 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 10000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:08:49.600 : Number of train images: 3,450, iters: 1,150
22-05-10 21:15:38.627 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 10000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:15:39.586 : Number of train images: 3,450, iters: 1,150
22-05-10 21:33:54.876 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 10000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:33:55.778 : Number of train images: 3,450, iters: 1,150
22-05-10 21:41:20.713 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 10000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:41:21.705 : Number of train images: 3,450, iters: 1,150
22-05-10 21:41:24.707 : 
Networks name: USRNet_ST
Params number: 3624844
Net structure:
USRNet_ST(
  (d): DataNet()
  (p): ResUNet(
    (m_head): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (m_down1): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down2): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down3): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (sw_res1): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (2): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (3): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res2): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (2): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (3): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res3): SwinIR(
      (conv_first): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (2): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (3): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res4): SwinIR(
      (conv_first): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (2): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (3): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=6
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (4): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (5): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (m_up3): Sequential(
      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up2): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up1): Sequential(
      (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_tail): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (h): HyPaNet(
    (mlp): Sequential(
      (0): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (3): ReLU(inplace=True)
      (4): Conv2d(32, 12, kernel_size=(1, 1), stride=(1, 1))
      (5): Softplus(beta=1, threshold=20)
    )
  )
)

22-05-10 21:41:24.941 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.002 | -0.084 |  0.102 |  0.033 | torch.Size([16, 4, 3, 3]) || p.m_head.weight
 | -0.000 | -0.054 |  0.048 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.0.weight
 | -0.000 | -0.056 |  0.054 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.2.weight
 |  0.000 | -0.058 |  0.059 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.0.weight
 |  0.000 | -0.060 |  0.052 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.2.weight
 |  0.000 | -0.071 |  0.089 |  0.025 | torch.Size([32, 16, 2, 2]) || p.m_down1.2.weight
 | -0.000 | -0.042 |  0.046 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.0.weight
 |  0.000 | -0.048 |  0.049 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.2.weight
 |  0.000 | -0.045 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.0.weight
 | -0.000 | -0.044 |  0.042 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.2.weight
 | -0.000 | -0.069 |  0.076 |  0.018 | torch.Size([64, 32, 2, 2]) || p.m_down2.2.weight
 | -0.000 | -0.038 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.0.weight
 | -0.000 | -0.036 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.2.weight
 | -0.000 | -0.036 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.0.weight
 |  0.000 | -0.030 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.2.weight
 | -0.000 | -0.052 |  0.050 |  0.013 | torch.Size([64, 64, 2, 2]) || p.m_down3.2.weight
 | -0.000 | -0.034 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.bias
 | -0.002 | -0.053 |  0.038 |  0.018 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.054 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.087 |  0.088 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.058 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.065 |  0.070 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.046 |  0.040 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.062 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.085 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.065 |  0.064 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.075 |  0.072 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.058 |  0.070 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.055 |  0.056 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.080 |  0.090 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.062 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.061 |  0.064 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.045 |  0.042 |  0.018 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.055 |  0.051 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.084 |  0.077 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.073 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.067 |  0.063 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.056 |  0.048 |  0.022 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.057 |  0.051 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.085 |  0.095 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.058 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.079 |  0.067 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.055 |  0.048 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.057 |  0.058 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.088 |  0.086 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.063 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.060 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.033 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.bias
 |  0.002 | -0.044 |  0.055 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.053 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.092 |  0.079 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.062 |  0.062 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.063 |  0.060 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.bias
 |  0.002 | -0.046 |  0.061 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.053 |  0.049 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.086 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.065 |  0.063 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.061 |  0.062 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.048 |  0.062 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.053 |  0.050 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.096 |  0.084 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.063 |  0.068 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.066 |  0.060 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.052 |  0.040 |  0.018 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.054 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.084 |  0.087 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.059 |  0.072 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.062 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.norm1.bias
 |  0.002 | -0.042 |  0.072 |  0.024 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.053 |  0.054 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.qkv.bias
 |  0.001 | -0.100 |  0.097 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.068 |  0.072 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.065 |  0.066 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.norm1.bias
 | -0.002 | -0.053 |  0.052 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.053 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.096 |  0.099 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.058 |  0.074 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.067 |  0.062 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.034 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.norm1.bias
 |  0.003 | -0.047 |  0.055 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.050 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.089 |  0.080 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.062 |  0.075 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.063 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.041 |  0.043 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.055 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.095 |  0.085 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.064 |  0.063 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.067 |  0.061 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.norm1.bias
 | -0.004 | -0.048 |  0.050 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.051 |  0.054 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.085 |  0.079 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.065 |  0.072 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.063 |  0.069 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.046 |  0.050 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.057 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.090 |  0.117 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.070 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.060 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.057 |  0.053 |  0.022 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.057 |  0.058 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.001 | -0.091 |  0.077 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.065 |  0.070 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.066 |  0.064 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.067 |  0.047 |  0.022 | torch.Size([25, 4]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.054 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.082 |  0.089 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.063 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.061 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.036 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.2.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.069 |  0.050 |  0.022 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.053 |  0.049 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.092 |  0.089 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.060 |  0.057 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.063 |  0.060 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.050 |  0.034 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.053 |  0.063 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.089 |  0.074 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.069 |  0.064 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.071 |  0.074 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.norm1.bias
 |  0.004 | -0.042 |  0.069 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.057 |  0.058 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.100 |  0.086 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.063 |  0.062 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.061 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.037 |  0.044 |  0.016 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.058 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.083 |  0.092 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.068 |  0.058 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.072 |  0.064 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.043 |  0.046 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.061 |  0.062 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.093 |  0.072 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.061 |  0.071 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.081 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.039 |  0.051 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.052 |  0.062 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.090 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.069 |  0.067 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.071 |  0.058 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.033 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.3.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.norm.bias
 |  0.000 | -0.032 |  0.030 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_after_body.bias
 | -0.000 | -0.033 |  0.031 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_before_upsample.0.bias
 |  0.000 | -0.044 |  0.035 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res1.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res1.upsample.0.bias
 | -0.000 | -0.037 |  0.038 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_last.bias
 |  0.000 | -0.034 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.048 |  0.063 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.057 |  0.059 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.081 |  0.098 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.066 |  0.057 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.065 |  0.063 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.bias
 | -0.002 | -0.045 |  0.044 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.052 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.083 |  0.096 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.062 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.066 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.048 |  0.051 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.054 |  0.049 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.093 |  0.081 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.058 |  0.065 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.063 |  0.066 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.062 |  0.055 |  0.022 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.055 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.077 |  0.087 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.064 |  0.071 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.064 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.norm1.bias
 |  0.002 | -0.044 |  0.046 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.068 |  0.058 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.082 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.070 |  0.062 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.064 |  0.058 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.norm1.bias
 |  0.003 | -0.047 |  0.038 |  0.021 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.055 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.090 |  0.090 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.061 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.061 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.037 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.040 |  0.052 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.054 |  0.053 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.092 |  0.090 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.061 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.072 |  0.064 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.064 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.052 |  0.051 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.088 |  0.100 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.065 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.069 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.045 |  0.040 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.053 |  0.050 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.077 |  0.097 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.062 |  0.071 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.063 |  0.073 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.bias
 | -0.004 | -0.051 |  0.055 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.056 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.105 |  0.083 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.065 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.074 |  0.066 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.norm1.bias
 |  0.002 | -0.063 |  0.057 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.056 |  0.053 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.101 |  0.094 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.062 |  0.067 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.065 |  0.064 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.057 |  0.044 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.052 |  0.061 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.096 |  0.085 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.059 |  0.064 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.062 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.034 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.051 |  0.070 |  0.023 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.054 |  0.062 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.087 |  0.106 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.061 |  0.072 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.067 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.norm1.bias
 | -0.006 | -0.055 |  0.057 |  0.025 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.053 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.089 |  0.097 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.063 |  0.063 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.069 |  0.073 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.051 |  0.048 |  0.021 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.052 |  0.056 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.082 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.070 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.072 |  0.074 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.045 |  0.058 |  0.021 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.054 |  0.056 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.090 |  0.089 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.070 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.066 |  0.060 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.norm1.bias
 | -0.002 | -0.044 |  0.041 |  0.017 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.051 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.090 |  0.079 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.068 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.067 |  0.066 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.045 |  0.051 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.054 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.qkv.bias
 |  0.001 | -0.083 |  0.089 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.068 |  0.064 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.2.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.068 |  0.062 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.038 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.2.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.057 |  0.044 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.053 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.078 |  0.097 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.059 |  0.063 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.063 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.norm1.bias
 | -0.005 | -0.051 |  0.042 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.059 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.074 |  0.084 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.063 |  0.069 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.061 |  0.061 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.043 |  0.050 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.055 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.079 |  0.089 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.075 |  0.057 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.064 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.067 |  0.056 |  0.022 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.057 |  0.059 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.100 |  0.085 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.061 |  0.075 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.066 |  0.063 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.norm1.bias
 |  0.003 | -0.039 |  0.049 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.059 |  0.054 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.086 |  0.095 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.061 |  0.068 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.066 |  0.070 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.061 |  0.045 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.056 |  0.059 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.096 |  0.085 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.065 |  0.070 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.3.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.062 |  0.079 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.034 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.3.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.norm.bias
 | -0.000 | -0.033 |  0.031 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_after_body.bias
 |  0.000 | -0.032 |  0.031 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_before_upsample.0.bias
 | -0.000 | -0.039 |  0.036 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res2.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res2.upsample.0.bias
 | -0.000 | -0.033 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_last.bias
 | -0.000 | -0.045 |  0.041 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.bias
 |  0.002 | -0.043 |  0.043 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.001 | -0.063 |  0.077 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.120 |  0.094 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.077 |  0.077 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.077 |  0.078 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.bias
 |  0.002 | -0.043 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.064 |  0.077 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.123 |  0.112 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.077 |  0.082 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.085 |  0.080 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.bias
 |  0.004 | -0.043 |  0.044 |  0.018 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.071 |  0.073 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.001 | -0.110 |  0.102 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.bias
 | -0.001 | -0.082 |  0.084 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.089 |  0.091 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.044 |  0.057 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.081 |  0.064 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.113 |  0.109 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.084 |  0.092 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.075 |  0.082 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.049 |  0.042 |  0.019 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.059 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.qkv.bias
 |  0.001 | -0.099 |  0.109 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.norm2.bias
 | -0.001 | -0.087 |  0.073 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.076 |  0.086 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.norm1.bias
 | -0.003 | -0.060 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.067 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.112 |  0.092 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.norm2.bias
 | -0.001 | -0.074 |  0.098 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.5.mlp.fc1.bias
 |  0.001 | -0.092 |  0.082 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.048 |  0.048 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.bias
 | -0.002 | -0.060 |  0.040 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.077 |  0.076 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.001 | -0.098 |  0.103 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.072 |  0.087 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.073 |  0.084 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.047 |  0.057 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.075 |  0.075 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.001 | -0.108 |  0.114 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.085 |  0.105 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.078 |  0.078 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.bias
 |  0.003 | -0.047 |  0.063 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.066 |  0.066 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.002 | -0.101 |  0.110 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.bias
 | -0.001 | -0.075 |  0.075 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.001 | -0.079 |  0.073 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.057 |  0.055 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.082 |  0.065 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.108 |  0.101 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.074 |  0.090 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.084 |  0.087 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.049 |  0.062 |  0.022 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.001 | -0.075 |  0.074 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.qkv.bias
 |  0.001 | -0.095 |  0.107 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.norm2.bias
 |  0.001 | -0.086 |  0.085 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.087 |  0.083 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.norm1.bias
 | -0.002 | -0.053 |  0.056 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.065 |  0.075 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.qkv.bias
 |  0.001 | -0.120 |  0.105 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.norm2.bias
 |  0.001 | -0.079 |  0.074 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.090 |  0.089 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.042 |  0.051 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.049 |  0.056 |  0.018 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.069 |  0.079 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.003 | -0.101 |  0.100 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.093 |  0.077 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.100 |  0.083 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.norm1.bias
 |  0.002 | -0.042 |  0.049 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.084 |  0.071 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.106 |  0.126 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.080 |  0.082 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.083 |  0.080 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.norm1.bias
 |  0.002 | -0.043 |  0.037 |  0.017 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.066 |  0.068 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.104 |  0.124 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.norm2.bias
 | -0.001 | -0.097 |  0.079 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.079 |  0.080 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.norm1.bias
 |  0.003 | -0.053 |  0.062 |  0.023 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.065 |  0.066 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.107 |  0.099 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.097 |  0.083 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.072 |  0.083 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.norm1.bias
 | -0.002 | -0.051 |  0.049 |  0.018 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.068 |  0.072 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.107 |  0.109 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.083 |  0.081 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.079 |  0.095 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.norm1.bias
 |  0.002 | -0.055 |  0.049 |  0.022 | torch.Size([25, 4]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.001 | -0.083 |  0.062 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.qkv.bias
 |  0.001 | -0.118 |  0.102 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.norm2.bias
 |  0.001 | -0.086 |  0.087 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.081 |  0.085 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.039 |  0.050 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.2.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.norm1.bias
 | -0.002 | -0.053 |  0.042 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.084 |  0.068 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.001 | -0.110 |  0.114 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.086 |  0.086 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.083 |  0.084 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.norm1.bias
 | -0.002 | -0.062 |  0.048 |  0.022 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.061 |  0.066 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.114 |  0.128 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.norm2.bias
 | -0.001 | -0.090 |  0.072 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.078 |  0.075 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.048 |  0.058 |  0.022 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.071 |  0.066 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.114 |  0.117 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.080 |  0.069 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.081 |  0.074 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.norm1.bias
 | -0.003 | -0.058 |  0.043 |  0.018 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.relative_position_index
 |  0.001 | -0.071 |  0.080 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.001 | -0.113 |  0.114 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.073 |  0.086 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.094 |  0.071 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.062 |  0.054 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.072 |  0.080 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.112 |  0.115 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.norm2.bias
 | -0.001 | -0.084 |  0.084 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.001 | -0.083 |  0.097 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.064 |  0.043 |  0.018 | torch.Size([25, 4]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.070 |  0.065 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.001 | -0.098 |  0.099 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.079 |  0.080 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.078 |  0.080 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.042 |  0.043 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.3.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.norm.bias
 |  0.000 | -0.046 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_after_body.bias
 | -0.000 | -0.044 |  0.044 |  0.012 | torch.Size([64, 32, 3, 3]) || p.sw_res3.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.conv_before_upsample.0.bias
 | -0.000 | -0.040 |  0.035 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res3.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res3.upsample.0.bias
 |  0.000 | -0.033 |  0.036 |  0.008 | torch.Size([32, 64, 3, 3]) || p.sw_res3.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_last.bias
 | -0.000 | -0.062 |  0.061 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.043 |  0.047 |  0.016 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.001 | -0.080 |  0.079 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.004 | -0.122 |  0.132 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.bias
 | -0.001 | -0.104 |  0.106 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.126 |  0.101 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.037 |  0.039 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.074 |  0.076 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.002 | -0.111 |  0.125 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.bias
 | -0.002 | -0.115 |  0.097 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.003 | -0.082 |  0.100 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.bias
 |  0.005 | -0.050 |  0.048 |  0.018 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.001 | -0.081 |  0.072 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.124 |  0.170 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.bias
 | -0.003 | -0.104 |  0.089 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.002 | -0.104 |  0.103 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.bias
 | -0.002 | -0.043 |  0.051 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.085 |  0.097 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.005 | -0.118 |  0.120 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.bias
 | -0.002 | -0.098 |  0.100 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.102 |  0.106 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.norm1.bias
 | -0.003 | -0.058 |  0.055 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.002 | -0.083 |  0.097 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.001 | -0.150 |  0.145 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.norm2.bias
 |  0.002 | -0.100 |  0.094 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.002 | -0.111 |  0.100 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.norm1.bias
 |  0.004 | -0.040 |  0.058 |  0.022 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.094 |  0.110 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.116 |  0.126 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.norm2.bias
 | -0.001 | -0.116 |  0.091 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.096 |  0.102 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.052 |  0.056 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.052 |  0.049 |  0.022 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.003 | -0.099 |  0.086 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.004 | -0.130 |  0.132 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.104 |  0.092 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.003 | -0.103 |  0.105 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.047 |  0.043 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.001 | -0.082 |  0.078 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.004 | -0.145 |  0.134 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.087 |  0.101 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.001 | -0.108 |  0.094 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.049 |  0.069 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.068 |  0.082 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.130 |  0.108 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.bias
 |  0.001 | -0.115 |  0.100 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.002 | -0.103 |  0.094 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.044 |  0.042 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.001 | -0.092 |  0.088 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.003 | -0.124 |  0.120 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.123 |  0.089 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.124 |  0.101 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.048 |  0.046 |  0.018 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.001 | -0.081 |  0.087 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.002 | -0.109 |  0.151 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.norm2.bias
 | -0.003 | -0.114 |  0.106 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.4.mlp.fc1.bias
 | -0.001 | -0.098 |  0.107 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.norm1.bias
 |  0.002 | -0.045 |  0.057 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.002 | -0.100 |  0.078 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.005 | -0.115 |  0.137 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.norm2.bias
 | -0.001 | -0.114 |  0.097 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.5.mlp.fc1.bias
 | -0.001 | -0.117 |  0.087 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.054 |  0.065 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.045 |  0.035 |  0.017 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.084 |  0.080 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.123 |  0.118 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.norm2.bias
 |  0.004 | -0.106 |  0.095 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.113 |  0.092 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.057 |  0.064 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.092 |  0.094 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.154 |  0.128 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.106 |  0.102 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.002 | -0.102 |  0.089 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.043 |  0.036 |  0.017 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.095 |  0.097 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.110 |  0.123 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.norm2.bias
 | -0.002 | -0.122 |  0.126 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.001 | -0.102 |  0.084 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.045 |  0.055 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.092 |  0.091 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.002 | -0.139 |  0.124 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.091 |  0.089 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.004 | -0.091 |  0.106 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.norm1.bias
 | -0.003 | -0.045 |  0.043 |  0.018 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.082 |  0.087 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.001 | -0.158 |  0.158 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.norm2.bias
 | -0.003 | -0.097 |  0.103 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.002 | -0.095 |  0.092 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.046 |  0.059 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.001 | -0.080 |  0.089 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.003 | -0.130 |  0.125 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.111 |  0.129 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.104 |  0.110 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.001 | -0.058 |  0.059 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.2.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.057 |  0.059 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.002 | -0.082 |  0.091 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.002 | -0.143 |  0.131 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.105 |  0.102 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.0.mlp.fc1.bias
 | -0.001 | -0.100 |  0.101 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.039 |  0.039 |  0.016 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.relative_position_index
 |  0.001 | -0.076 |  0.101 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.005 | -0.130 |  0.133 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.087 |  0.093 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.107 |  0.116 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.norm1.bias
 |  0.003 | -0.042 |  0.053 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.relative_position_index
 | -0.002 | -0.094 |  0.079 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.002 | -0.116 |  0.123 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.norm2.bias
 |  0.002 | -0.109 |  0.088 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.001 | -0.087 |  0.092 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.norm1.bias
 |  0.003 | -0.052 |  0.058 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.075 |  0.085 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.002 | -0.130 |  0.121 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.107 |  0.095 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.093 |  0.091 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.056 |  0.049 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.001 | -0.087 |  0.091 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.110 |  0.129 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.norm2.bias
 |  0.001 | -0.103 |  0.115 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.110 |  0.100 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.048 |  0.056 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.086 |  0.083 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.004 | -0.131 |  0.137 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.114 |  0.105 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.002 | -0.085 |  0.099 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.055 |  0.061 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.3.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.norm.bias
 |  0.001 | -0.059 |  0.056 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_after_body.bias
 | -0.000 | -0.069 |  0.069 |  0.017 | torch.Size([64, 16, 3, 3]) || p.sw_res4.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res4.conv_before_upsample.0.bias
 | -0.000 | -0.036 |  0.038 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res4.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res4.upsample.0.bias
 | -0.000 | -0.032 |  0.032 |  0.008 | torch.Size([16, 64, 3, 3]) || p.sw_res4.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_last.bias
 | -0.000 | -0.047 |  0.053 |  0.013 | torch.Size([64, 64, 2, 2]) || p.m_up3.0.weight
 | -0.000 | -0.032 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.0.weight
 |  0.000 | -0.035 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.2.weight
 |  0.000 | -0.035 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.0.weight
 | -0.000 | -0.033 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.2.weight
 |  0.000 | -0.064 |  0.062 |  0.018 | torch.Size([64, 32, 2, 2]) || p.m_up2.0.weight
 |  0.000 | -0.050 |  0.044 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.0.weight
 | -0.000 | -0.043 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.2.weight
 |  0.000 | -0.041 |  0.047 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.0.weight
 |  0.000 | -0.045 |  0.044 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.2.weight
 | -0.000 | -0.084 |  0.079 |  0.025 | torch.Size([32, 16, 2, 2]) || p.m_up1.0.weight
 | -0.000 | -0.059 |  0.058 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.0.weight
 |  0.000 | -0.059 |  0.055 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.2.weight
 |  0.000 | -0.050 |  0.061 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.0.weight
 |  0.000 | -0.058 |  0.064 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.2.weight
 |  0.000 | -0.055 |  0.053 |  0.017 | torch.Size([3, 16, 3, 3]) || p.m_tail.weight
 |  0.006 | -0.078 |  0.070 |  0.035 | torch.Size([32, 2, 1, 1]) || h.mlp.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || h.mlp.0.bias
 |  0.000 | -0.106 |  0.094 |  0.035 | torch.Size([32, 32, 1, 1]) || h.mlp.2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || h.mlp.2.bias
 | -0.001 | -0.096 |  0.115 |  0.035 | torch.Size([12, 32, 1, 1]) || h.mlp.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([12]) || h.mlp.4.bias

22-05-10 21:54:24.963 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 20000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:54:25.902 : Number of train images: 3,450, iters: 1,150
22-05-10 21:54:28.351 : 
Networks name: USRNet_ST
Params number: 2204204
Net structure:
USRNet_ST(
  (d): DataNet()
  (p): ResUNet(
    (m_head): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (m_down1): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down2): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down3): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (sw_res1): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res2): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res3): SwinIR(
      (conv_first): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res4): SwinIR(
      (conv_first): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (m_up3): Sequential(
      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up2): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up1): Sequential(
      (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_tail): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (h): HyPaNet(
    (mlp): Sequential(
      (0): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (3): ReLU(inplace=True)
      (4): Conv2d(32, 12, kernel_size=(1, 1), stride=(1, 1))
      (5): Softplus(beta=1, threshold=20)
    )
  )
)

22-05-10 21:54:28.438 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.001 | -0.107 |  0.109 |  0.033 | torch.Size([16, 4, 3, 3]) || p.m_head.weight
 |  0.000 | -0.052 |  0.062 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.0.weight
 | -0.000 | -0.055 |  0.059 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.2.weight
 |  0.000 | -0.058 |  0.050 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.0.weight
 | -0.000 | -0.075 |  0.055 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.2.weight
 | -0.000 | -0.084 |  0.083 |  0.025 | torch.Size([32, 16, 2, 2]) || p.m_down1.2.weight
 |  0.000 | -0.040 |  0.043 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.0.weight
 |  0.000 | -0.051 |  0.047 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.2.weight
 |  0.000 | -0.043 |  0.043 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.0.weight
 |  0.000 | -0.039 |  0.047 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.2.weight
 |  0.000 | -0.076 |  0.060 |  0.018 | torch.Size([64, 32, 2, 2]) || p.m_down2.2.weight
 |  0.000 | -0.034 |  0.037 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.0.weight
 | -0.000 | -0.041 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.2.weight
 |  0.000 | -0.033 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.0.weight
 |  0.000 | -0.042 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.2.weight
 |  0.000 | -0.048 |  0.044 |  0.012 | torch.Size([64, 64, 2, 2]) || p.m_down3.2.weight
 |  0.000 | -0.033 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.bias
 |  0.003 | -0.047 |  0.062 |  0.022 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.053 |  0.059 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.080 |  0.093 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.058 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.058 |  0.063 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.bias
 | -0.006 | -0.044 |  0.050 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.052 |  0.056 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.089 |  0.087 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.072 |  0.071 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.067 |  0.067 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.038 |  0.049 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.051 |  0.053 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.079 |  0.092 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.069 |  0.064 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.065 |  0.073 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.040 |  0.047 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.053 |  0.051 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.082 |  0.085 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.061 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.064 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.037 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.040 |  0.043 |  0.018 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.053 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.095 |  0.091 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.070 |  0.069 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.059 |  0.061 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.048 |  0.053 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.062 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.086 |  0.090 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.066 |  0.062 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.060 |  0.060 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.bias
 | -0.003 | -0.077 |  0.044 |  0.021 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.054 |  0.058 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.091 |  0.078 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.062 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.060 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.040 |  0.054 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.052 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.085 |  0.097 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.064 |  0.058 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.061 |  0.071 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.034 |  0.031 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.norm.bias
 | -0.000 | -0.032 |  0.031 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_after_body.bias
 |  0.000 | -0.034 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_before_upsample.0.bias
 |  0.000 | -0.034 |  0.034 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res1.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res1.upsample.0.bias
 | -0.000 | -0.034 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_last.bias
 | -0.000 | -0.031 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.bias
 | -0.002 | -0.065 |  0.064 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.057 |  0.061 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.092 |  0.092 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.063 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.064 |  0.070 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.bias
 |  0.002 | -0.048 |  0.042 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.047 |  0.051 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.097 |  0.083 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.064 |  0.056 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.070 |  0.071 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.bias
 |  0.003 | -0.058 |  0.067 |  0.024 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.059 |  0.056 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.094 |  0.097 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.069 |  0.067 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.065 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.049 |  0.057 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.050 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.084 |  0.083 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.066 |  0.064 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.066 |  0.070 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.035 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.041 |  0.047 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.056 |  0.059 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.084 |  0.081 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.068 |  0.068 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.063 |  0.069 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.057 |  0.045 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.050 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.078 |  0.095 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.072 |  0.072 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.066 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.049 |  0.042 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.053 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.076 |  0.094 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.064 |  0.064 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.067 |  0.064 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.047 |  0.049 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.053 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.081 |  0.095 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.065 |  0.066 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.074 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.033 |  0.036 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.norm.bias
 |  0.000 | -0.038 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_after_body.bias
 | -0.000 | -0.033 |  0.037 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_before_upsample.0.bias
 | -0.000 | -0.034 |  0.035 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res2.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res2.upsample.0.bias
 |  0.000 | -0.035 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_last.bias
 |  0.000 | -0.044 |  0.048 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.062 |  0.064 |  0.023 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.069 |  0.061 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.100 |  0.109 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.bias
 |  0.001 | -0.077 |  0.083 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.082 |  0.076 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.048 |  0.049 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.071 |  0.074 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.097 |  0.086 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.075 |  0.077 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.001 | -0.080 |  0.076 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.046 |  0.046 |  0.019 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.067 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.129 |  0.100 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.079 |  0.092 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.082 |  0.071 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.041 |  0.058 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.071 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.001 | -0.097 |  0.116 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.088 |  0.081 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.084 |  0.086 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.042 |  0.040 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.bias
 |  0.002 | -0.053 |  0.042 |  0.022 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.076 |  0.072 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.001 | -0.099 |  0.116 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.094 |  0.077 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.083 |  0.084 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.039 |  0.073 |  0.019 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.071 |  0.072 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.096 |  0.104 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.bias
 | -0.001 | -0.081 |  0.099 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.001 | -0.085 |  0.078 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.050 |  0.044 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.079 |  0.067 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.098 |  0.104 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.085 |  0.071 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.001 | -0.078 |  0.098 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.063 |  0.037 |  0.019 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.001 | -0.068 |  0.077 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.003 | -0.103 |  0.107 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.080 |  0.082 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.001 | -0.087 |  0.077 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.045 |  0.044 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.norm.bias
 | -0.000 | -0.048 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_after_body.bias
 | -0.000 | -0.045 |  0.055 |  0.012 | torch.Size([64, 32, 3, 3]) || p.sw_res3.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.conv_before_upsample.0.bias
 | -0.000 | -0.037 |  0.036 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res3.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res3.upsample.0.bias
 |  0.000 | -0.033 |  0.035 |  0.008 | torch.Size([32, 64, 3, 3]) || p.sw_res3.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_last.bias
 |  0.000 | -0.061 |  0.051 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.047 |  0.053 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.003 | -0.086 |  0.100 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.004 | -0.142 |  0.126 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.086 |  0.095 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.100 |  0.084 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.bias
 |  0.002 | -0.049 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.094 |  0.099 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.127 |  0.114 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.bias
 | -0.002 | -0.114 |  0.084 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.002 | -0.088 |  0.106 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.bias
 |  0.004 | -0.040 |  0.048 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.102 |  0.074 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.002 | -0.152 |  0.128 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.bias
 |  0.002 | -0.089 |  0.117 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.003 | -0.090 |  0.102 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.061 |  0.048 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.001 | -0.096 |  0.078 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.004 | -0.124 |  0.114 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.106 |  0.089 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.004 | -0.090 |  0.098 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.054 |  0.058 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.bias
 |  0.004 | -0.037 |  0.045 |  0.018 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.079 |  0.082 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.007 | -0.118 |  0.124 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.088 |  0.097 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.003 | -0.116 |  0.097 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.041 |  0.049 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.001 | -0.088 |  0.080 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.124 |  0.129 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.109 |  0.098 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.002 | -0.106 |  0.107 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.040 |  0.043 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.083 |  0.096 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.004 | -0.131 |  0.129 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.bias
 | -0.001 | -0.091 |  0.094 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.120 |  0.094 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.bias
 | -0.004 | -0.066 |  0.042 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.002 | -0.102 |  0.087 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.004 | -0.119 |  0.115 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.113 |  0.104 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.002 | -0.106 |  0.099 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.062 |  0.061 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.norm.bias
 |  0.000 | -0.064 |  0.059 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_after_body.bias
 |  0.000 | -0.061 |  0.063 |  0.017 | torch.Size([64, 16, 3, 3]) || p.sw_res4.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res4.conv_before_upsample.0.bias
 | -0.000 | -0.039 |  0.041 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res4.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res4.upsample.0.bias
 | -0.000 | -0.033 |  0.029 |  0.008 | torch.Size([16, 64, 3, 3]) || p.sw_res4.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_last.bias
 |  0.000 | -0.061 |  0.045 |  0.013 | torch.Size([64, 64, 2, 2]) || p.m_up3.0.weight
 | -0.000 | -0.035 |  0.036 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.0.weight
 | -0.000 | -0.034 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.2.weight
 |  0.000 | -0.036 |  0.045 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.0.weight
 | -0.000 | -0.036 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.2.weight
 |  0.000 | -0.059 |  0.061 |  0.018 | torch.Size([64, 32, 2, 2]) || p.m_up2.0.weight
 |  0.000 | -0.039 |  0.050 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.0.weight
 |  0.000 | -0.048 |  0.044 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.2.weight
 | -0.000 | -0.043 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.0.weight
 |  0.000 | -0.041 |  0.046 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.2.weight
 | -0.000 | -0.103 |  0.076 |  0.025 | torch.Size([32, 16, 2, 2]) || p.m_up1.0.weight
 |  0.000 | -0.064 |  0.049 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.0.weight
 | -0.000 | -0.060 |  0.058 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.2.weight
 |  0.000 | -0.056 |  0.052 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.0.weight
 | -0.000 | -0.066 |  0.057 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.2.weight
 | -0.001 | -0.053 |  0.040 |  0.017 | torch.Size([3, 16, 3, 3]) || p.m_tail.weight
 | -0.001 | -0.075 |  0.084 |  0.036 | torch.Size([32, 2, 1, 1]) || h.mlp.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || h.mlp.0.bias
 |  0.002 | -0.115 |  0.099 |  0.035 | torch.Size([32, 32, 1, 1]) || h.mlp.2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || h.mlp.2.bias
 | -0.002 | -0.098 |  0.090 |  0.035 | torch.Size([12, 32, 1, 1]) || h.mlp.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([12]) || h.mlp.4.bias

22-05-10 21:55:06.002 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 20000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-10 21:55:08.406 : Number of train images: 3,450, iters: 1,150
22-05-10 21:55:19.085 : 
Networks name: USRNet_ST
Params number: 2204204
Net structure:
USRNet_ST(
  (d): DataNet()
  (p): ResUNet(
    (m_head): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (m_down1): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down2): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down3): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (sw_res1): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res2): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res3): SwinIR(
      (conv_first): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res4): SwinIR(
      (conv_first): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (m_up3): Sequential(
      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up2): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up1): Sequential(
      (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_tail): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (h): HyPaNet(
    (mlp): Sequential(
      (0): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (3): ReLU(inplace=True)
      (4): Conv2d(32, 12, kernel_size=(1, 1), stride=(1, 1))
      (5): Softplus(beta=1, threshold=20)
    )
  )
)

22-05-10 21:55:19.172 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.001 | -0.113 |  0.090 |  0.033 | torch.Size([16, 4, 3, 3]) || p.m_head.weight
 | -0.001 | -0.068 |  0.062 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.0.weight
 |  0.000 | -0.057 |  0.052 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.2.weight
 | -0.000 | -0.056 |  0.057 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.0.weight
 |  0.001 | -0.066 |  0.061 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.2.weight
 |  0.000 | -0.088 |  0.091 |  0.025 | torch.Size([32, 16, 2, 2]) || p.m_down1.2.weight
 |  0.000 | -0.042 |  0.044 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.0.weight
 |  0.000 | -0.042 |  0.044 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.2.weight
 | -0.000 | -0.046 |  0.040 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.0.weight
 | -0.000 | -0.042 |  0.046 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.2.weight
 | -0.000 | -0.060 |  0.058 |  0.018 | torch.Size([64, 32, 2, 2]) || p.m_down2.2.weight
 | -0.000 | -0.037 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.0.weight
 | -0.000 | -0.036 |  0.030 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.2.weight
 |  0.000 | -0.035 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.0.weight
 |  0.000 | -0.034 |  0.040 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.2.weight
 | -0.000 | -0.048 |  0.052 |  0.012 | torch.Size([64, 64, 2, 2]) || p.m_down3.2.weight
 |  0.000 | -0.034 |  0.037 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.046 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.051 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.107 |  0.088 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.064 |  0.062 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.065 |  0.067 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.bias
 |  0.003 | -0.037 |  0.045 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.053 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.080 |  0.100 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.063 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.073 |  0.070 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.053 |  0.051 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.055 |  0.054 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.080 |  0.079 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.064 |  0.063 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.062 |  0.075 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.bias
 |  0.003 | -0.051 |  0.051 |  0.017 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.051 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.090 |  0.083 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.061 |  0.059 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.061 |  0.061 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.034 |  0.036 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.052 |  0.059 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.054 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.001 | -0.078 |  0.081 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.063 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.059 |  0.074 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.043 |  0.043 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.053 |  0.061 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.085 |  0.091 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.058 |  0.070 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.077 |  0.066 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.054 |  0.053 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.051 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.092 |  0.087 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.071 |  0.065 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.067 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.bias
 | -0.002 | -0.049 |  0.076 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.054 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.105 |  0.079 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.067 |  0.059 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.062 |  0.062 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.032 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.norm.bias
 | -0.000 | -0.034 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_after_body.bias
 |  0.000 | -0.037 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_before_upsample.0.bias
 | -0.000 | -0.036 |  0.036 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res1.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res1.upsample.0.bias
 | -0.000 | -0.033 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_last.bias
 | -0.000 | -0.033 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.bias
 |  0.002 | -0.066 |  0.047 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.063 |  0.051 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.075 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.069 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.066 |  0.074 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.bias
 | -0.006 | -0.062 |  0.046 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.056 |  0.058 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.084 |  0.078 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.062 |  0.058 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.071 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.043 |  0.045 |  0.017 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.058 |  0.054 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.079 |  0.083 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.059 |  0.065 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.064 |  0.062 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.051 |  0.046 |  0.021 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.059 |  0.076 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.078 |  0.098 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.066 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.061 |  0.067 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.031 |  0.036 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.047 |  0.056 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.052 |  0.061 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.084 |  0.088 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.060 |  0.069 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.066 |  0.063 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.bias
 | -0.002 | -0.036 |  0.050 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.053 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.090 |  0.098 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.063 |  0.082 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.068 |  0.061 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.053 |  0.046 |  0.023 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.061 |  0.056 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.080 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.062 |  0.069 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.066 |  0.060 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.038 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.059 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.083 |  0.086 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.062 |  0.069 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.062 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.033 |  0.040 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.norm.bias
 |  0.000 | -0.040 |  0.039 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_after_body.bias
 | -0.000 | -0.030 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_before_upsample.0.bias
 | -0.000 | -0.035 |  0.036 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res2.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res2.upsample.0.bias
 |  0.000 | -0.033 |  0.037 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_last.bias
 |  0.000 | -0.052 |  0.043 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.bias
 | -0.002 | -0.044 |  0.062 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.078 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.001 | -0.096 |  0.102 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.079 |  0.089 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.080 |  0.083 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.067 |  0.045 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.084 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.106 |  0.127 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.077 |  0.080 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.070 |  0.074 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.041 |  0.055 |  0.019 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.066 |  0.076 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.001 | -0.097 |  0.096 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.bias
 | -0.001 | -0.076 |  0.075 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.001 | -0.075 |  0.091 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.046 |  0.065 |  0.022 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.065 |  0.067 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.113 |  0.112 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.076 |  0.079 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.001 | -0.081 |  0.076 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.045 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.bias
 |  0.002 | -0.051 |  0.059 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.061 |  0.080 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.110 |  0.118 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.bias
 | -0.001 | -0.079 |  0.083 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.077 |  0.099 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.051 |  0.051 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.068 |  0.067 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.119 |  0.093 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.074 |  0.080 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.083 |  0.087 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.052 |  0.061 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.070 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.093 |  0.112 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.072 |  0.094 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.094 |  0.078 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.bias
 | -0.002 | -0.054 |  0.043 |  0.017 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.071 |  0.070 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.001 | -0.096 |  0.107 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.077 |  0.087 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.079 |  0.081 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.040 |  0.043 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.norm.bias
 |  0.000 | -0.047 |  0.048 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_after_body.bias
 | -0.000 | -0.045 |  0.045 |  0.012 | torch.Size([64, 32, 3, 3]) || p.sw_res3.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.conv_before_upsample.0.bias
 | -0.000 | -0.036 |  0.034 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res3.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res3.upsample.0.bias
 | -0.000 | -0.033 |  0.033 |  0.008 | torch.Size([32, 64, 3, 3]) || p.sw_res3.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_last.bias
 | -0.000 | -0.053 |  0.053 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.056 |  0.056 |  0.023 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.087 |  0.090 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.003 | -0.110 |  0.123 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.106 |  0.090 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.107 |  0.113 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.bias
 |  0.002 | -0.034 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.001 | -0.088 |  0.082 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.007 | -0.130 |  0.120 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.bias
 | -0.001 | -0.091 |  0.093 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.089 |  0.102 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.047 |  0.046 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.092 |  0.075 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.002 | -0.114 |  0.129 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.bias
 |  0.002 | -0.098 |  0.104 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.001 | -0.101 |  0.098 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.045 |  0.049 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.002 | -0.076 |  0.074 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.008 | -0.122 |  0.111 |  0.049 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.122 |  0.113 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.002 | -0.109 |  0.092 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.052 |  0.052 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.bias
 | -0.003 | -0.043 |  0.042 |  0.017 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.086 |  0.095 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.001 | -0.129 |  0.136 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.bias
 |  0.001 | -0.100 |  0.100 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.102 |  0.101 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.050 |  0.057 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.104 |  0.098 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.129 |  0.125 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.bias
 | -0.002 | -0.117 |  0.098 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.109 |  0.104 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.bias
 |  0.002 | -0.041 |  0.055 |  0.018 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.003 | -0.086 |  0.088 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.120 |  0.131 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.bias
 |  0.001 | -0.103 |  0.097 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.001 | -0.100 |  0.091 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.062 |  0.054 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.001 | -0.093 |  0.090 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.003 | -0.127 |  0.115 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.bias
 | -0.001 | -0.136 |  0.095 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.088 |  0.090 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.055 |  0.054 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.norm.bias
 | -0.000 | -0.060 |  0.057 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_after_body.bias
 | -0.000 | -0.063 |  0.061 |  0.017 | torch.Size([64, 16, 3, 3]) || p.sw_res4.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res4.conv_before_upsample.0.bias
 | -0.000 | -0.034 |  0.035 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res4.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res4.upsample.0.bias
 | -0.000 | -0.033 |  0.028 |  0.008 | torch.Size([16, 64, 3, 3]) || p.sw_res4.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_last.bias
 | -0.000 | -0.044 |  0.052 |  0.013 | torch.Size([64, 64, 2, 2]) || p.m_up3.0.weight
 |  0.000 | -0.036 |  0.031 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.0.weight
 |  0.000 | -0.031 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.2.weight
 | -0.000 | -0.031 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.0.weight
 | -0.000 | -0.036 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.2.weight
 |  0.000 | -0.061 |  0.068 |  0.018 | torch.Size([64, 32, 2, 2]) || p.m_up2.0.weight
 | -0.000 | -0.041 |  0.046 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.0.weight
 | -0.000 | -0.044 |  0.048 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.2.weight
 |  0.000 | -0.044 |  0.041 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.0.weight
 | -0.000 | -0.048 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.2.weight
 | -0.001 | -0.089 |  0.076 |  0.025 | torch.Size([32, 16, 2, 2]) || p.m_up1.0.weight
 |  0.000 | -0.059 |  0.061 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.0.weight
 |  0.000 | -0.062 |  0.060 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.2.weight
 |  0.000 | -0.056 |  0.048 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.0.weight
 |  0.000 | -0.054 |  0.050 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.2.weight
 | -0.000 | -0.051 |  0.048 |  0.017 | torch.Size([3, 16, 3, 3]) || p.m_tail.weight
 |  0.004 | -0.084 |  0.099 |  0.035 | torch.Size([32, 2, 1, 1]) || h.mlp.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || h.mlp.0.bias
 | -0.000 | -0.095 |  0.107 |  0.035 | torch.Size([32, 32, 1, 1]) || h.mlp.2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || h.mlp.2.bias
 | -0.001 | -0.115 |  0.094 |  0.035 | torch.Size([12, 32, 1, 1]) || h.mlp.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([12]) || h.mlp.4.bias

22-05-10 21:58:16.051 : <epoch:  0, iter:     200, lr:1.000e-04> G_loss: 2.120e-01 
22-05-10 22:01:09.922 : <epoch:  0, iter:     400, lr:1.000e-04> G_loss: 1.862e-01 
22-05-10 22:04:03.844 : <epoch:  0, iter:     600, lr:1.000e-04> G_loss: 1.394e-01 
22-05-10 22:06:57.884 : <epoch:  0, iter:     800, lr:1.000e-04> G_loss: 1.737e-01 
22-05-10 22:09:51.544 : <epoch:  0, iter:   1,000, lr:1.000e-04> G_loss: 1.462e-01 
22-05-10 22:12:49.696 : <epoch:  1, iter:   1,200, lr:1.000e-04> G_loss: 2.735e-01 
22-05-10 22:15:44.361 : <epoch:  1, iter:   1,400, lr:1.000e-04> G_loss: 1.994e-01 
22-05-10 22:18:38.638 : <epoch:  1, iter:   1,600, lr:1.000e-04> G_loss: 2.140e-01 
22-05-10 22:21:32.239 : <epoch:  1, iter:   1,800, lr:1.000e-04> G_loss: 1.972e-01 
22-05-10 22:24:25.988 : <epoch:  1, iter:   2,000, lr:1.000e-04> G_loss: 6.149e-02 
22-05-10 22:27:19.926 : <epoch:  1, iter:   2,200, lr:1.000e-04> G_loss: 7.116e-02 
22-05-10 22:30:15.593 : <epoch:  2, iter:   2,400, lr:1.000e-04> G_loss: 1.373e-01 
22-05-10 22:33:09.367 : <epoch:  2, iter:   2,600, lr:1.000e-04> G_loss: 1.300e-01 
22-05-10 22:36:03.316 : <epoch:  2, iter:   2,800, lr:1.000e-04> G_loss: 7.682e-02 
22-05-10 22:38:57.350 : <epoch:  2, iter:   3,000, lr:1.000e-04> G_loss: 7.946e-02 
22-05-10 22:41:51.469 : <epoch:  2, iter:   3,200, lr:1.000e-04> G_loss: 8.176e-02 
22-05-10 22:44:45.548 : <epoch:  2, iter:   3,400, lr:1.000e-04> G_loss: 5.347e-02 
22-05-10 22:47:42.113 : <epoch:  3, iter:   3,600, lr:1.000e-04> G_loss: 5.439e-02 
22-05-10 22:50:36.724 : <epoch:  3, iter:   3,800, lr:1.000e-04> G_loss: 5.406e-02 
22-05-10 22:53:32.228 : <epoch:  3, iter:   4,000, lr:1.000e-04> G_loss: 4.476e-02 
22-05-10 22:56:27.595 : <epoch:  3, iter:   4,200, lr:1.000e-04> G_loss: 6.061e-02 
22-05-10 22:59:22.540 : <epoch:  3, iter:   4,400, lr:1.000e-04> G_loss: 4.919e-02 
22-05-10 23:02:17.393 : <epoch:  3, iter:   4,600, lr:1.000e-04> G_loss: 6.682e-02 
22-05-10 23:05:14.527 : <epoch:  4, iter:   4,800, lr:1.000e-04> G_loss: 7.343e-02 
22-05-10 23:08:09.357 : <epoch:  4, iter:   5,000, lr:1.000e-04> G_loss: 3.755e-02 
22-05-10 23:08:09.358 : Saving the model.
22-05-10 23:11:06.297 : <epoch:  4, iter:   5,200, lr:1.000e-04> G_loss: 5.332e-02 
22-05-10 23:14:01.616 : <epoch:  4, iter:   5,400, lr:1.000e-04> G_loss: 4.115e-02 
22-05-10 23:16:57.466 : <epoch:  4, iter:   5,600, lr:1.000e-04> G_loss: 5.220e-02 
22-05-10 23:19:55.460 : <epoch:  5, iter:   5,800, lr:1.000e-04> G_loss: 6.315e-02 
22-05-10 23:22:51.619 : <epoch:  5, iter:   6,000, lr:1.000e-04> G_loss: 3.747e-02 
22-05-10 23:25:47.788 : <epoch:  5, iter:   6,200, lr:1.000e-04> G_loss: 6.518e-02 
22-05-10 23:28:44.226 : <epoch:  5, iter:   6,400, lr:1.000e-04> G_loss: 5.652e-02 
22-05-10 23:31:40.455 : <epoch:  5, iter:   6,600, lr:1.000e-04> G_loss: 6.511e-02 
22-05-10 23:34:36.324 : <epoch:  5, iter:   6,800, lr:1.000e-04> G_loss: 3.473e-02 
22-05-10 23:37:34.705 : <epoch:  6, iter:   7,000, lr:1.000e-04> G_loss: 3.092e-02 
22-05-10 23:40:30.875 : <epoch:  6, iter:   7,200, lr:1.000e-04> G_loss: 6.071e-02 
22-05-10 23:43:26.873 : <epoch:  6, iter:   7,400, lr:1.000e-04> G_loss: 3.609e-02 
22-05-10 23:46:23.143 : <epoch:  6, iter:   7,600, lr:1.000e-04> G_loss: 3.812e-02 
22-05-10 23:49:19.181 : <epoch:  6, iter:   7,800, lr:1.000e-04> G_loss: 6.638e-02 
22-05-10 23:52:14.848 : <epoch:  6, iter:   8,000, lr:1.000e-04> G_loss: 2.151e-02 
22-05-10 23:55:11.468 : <epoch:  7, iter:   8,200, lr:1.000e-04> G_loss: 3.049e-02 
22-05-10 23:58:06.372 : <epoch:  7, iter:   8,400, lr:1.000e-04> G_loss: 3.437e-02 
22-05-11 00:01:01.096 : <epoch:  7, iter:   8,600, lr:1.000e-04> G_loss: 4.790e-02 
22-05-11 00:03:55.753 : <epoch:  7, iter:   8,800, lr:1.000e-04> G_loss: 8.127e-02 
22-05-11 00:06:50.805 : <epoch:  7, iter:   9,000, lr:1.000e-04> G_loss: 3.658e-02 
22-05-11 00:09:45.561 : <epoch:  7, iter:   9,200, lr:1.000e-04> G_loss: 3.099e-02 
22-05-11 00:12:43.268 : <epoch:  8, iter:   9,400, lr:1.000e-04> G_loss: 3.936e-02 
22-05-11 00:15:38.961 : <epoch:  8, iter:   9,600, lr:1.000e-04> G_loss: 4.890e-02 
22-05-11 00:18:35.650 : <epoch:  8, iter:   9,800, lr:1.000e-04> G_loss: 4.451e-02 
22-05-11 00:21:31.398 : <epoch:  8, iter:  10,000, lr:1.000e-04> G_loss: 3.802e-02 
22-05-11 00:21:31.399 : Saving the model.
22-05-11 00:24:29.149 : <epoch:  8, iter:  10,200, lr:1.000e-04> G_loss: 3.826e-02 
22-05-11 00:27:27.027 : <epoch:  9, iter:  10,400, lr:1.000e-04> G_loss: 3.582e-02 
22-05-11 00:30:23.801 : <epoch:  9, iter:  10,600, lr:1.000e-04> G_loss: 4.155e-02 
22-05-11 00:33:20.874 : <epoch:  9, iter:  10,800, lr:1.000e-04> G_loss: 4.279e-02 
22-05-11 00:36:17.278 : <epoch:  9, iter:  11,000, lr:1.000e-04> G_loss: 3.458e-02 
22-05-11 00:39:13.255 : <epoch:  9, iter:  11,200, lr:1.000e-04> G_loss: 4.600e-02 
22-05-11 00:42:09.386 : <epoch:  9, iter:  11,400, lr:1.000e-04> G_loss: 3.282e-02 
22-05-11 00:45:08.013 : <epoch: 10, iter:  11,600, lr:1.000e-04> G_loss: 3.399e-02 
22-05-11 00:48:04.209 : <epoch: 10, iter:  11,800, lr:1.000e-04> G_loss: 4.459e-02 
22-05-11 00:51:00.789 : <epoch: 10, iter:  12,000, lr:1.000e-04> G_loss: 2.807e-02 
22-05-11 00:53:57.095 : <epoch: 10, iter:  12,200, lr:1.000e-04> G_loss: 4.666e-02 
22-05-11 00:56:54.931 : <epoch: 10, iter:  12,400, lr:1.000e-04> G_loss: 2.236e-02 
22-05-11 00:59:53.891 : <epoch: 10, iter:  12,600, lr:1.000e-04> G_loss: 4.646e-02 
22-05-11 01:02:55.498 : <epoch: 11, iter:  12,800, lr:1.000e-04> G_loss: 5.620e-02 
22-05-11 01:05:54.304 : <epoch: 11, iter:  13,000, lr:1.000e-04> G_loss: 2.653e-02 
22-05-11 01:08:53.578 : <epoch: 11, iter:  13,200, lr:1.000e-04> G_loss: 4.859e-02 
22-05-11 01:11:52.366 : <epoch: 11, iter:  13,400, lr:1.000e-04> G_loss: 7.580e-02 
22-05-11 01:14:50.976 : <epoch: 11, iter:  13,600, lr:1.000e-04> G_loss: 4.450e-02 
22-05-11 01:17:48.797 : <epoch: 11, iter:  13,800, lr:1.000e-04> G_loss: 5.251e-02 
22-05-11 01:20:48.002 : <epoch: 12, iter:  14,000, lr:1.000e-04> G_loss: 1.481e-02 
22-05-11 01:23:45.548 : <epoch: 12, iter:  14,200, lr:1.000e-04> G_loss: 2.222e-02 
22-05-11 01:26:44.149 : <epoch: 12, iter:  14,400, lr:1.000e-04> G_loss: 5.210e-02 
22-05-11 01:29:41.715 : <epoch: 12, iter:  14,600, lr:1.000e-04> G_loss: 5.818e-02 
22-05-11 01:32:39.450 : <epoch: 12, iter:  14,800, lr:1.000e-04> G_loss: 3.104e-02 
22-05-11 01:35:40.211 : <epoch: 13, iter:  15,000, lr:1.000e-04> G_loss: 1.961e-02 
22-05-11 01:35:40.213 : Saving the model.
22-05-11 01:38:40.747 : <epoch: 13, iter:  15,200, lr:1.000e-04> G_loss: 7.196e-02 
22-05-11 01:41:38.500 : <epoch: 13, iter:  15,400, lr:1.000e-04> G_loss: 6.465e-02 
22-05-11 01:44:35.521 : <epoch: 13, iter:  15,600, lr:1.000e-04> G_loss: 2.569e-02 
22-05-11 01:47:32.310 : <epoch: 13, iter:  15,800, lr:1.000e-04> G_loss: 5.204e-02 
22-05-11 01:50:29.462 : <epoch: 13, iter:  16,000, lr:1.000e-04> G_loss: 2.851e-02 
22-05-11 01:53:29.771 : <epoch: 14, iter:  16,200, lr:1.000e-04> G_loss: 4.482e-02 
22-05-11 01:56:26.900 : <epoch: 14, iter:  16,400, lr:1.000e-04> G_loss: 4.132e-02 
22-05-11 01:59:23.245 : <epoch: 14, iter:  16,600, lr:1.000e-04> G_loss: 2.570e-02 
22-05-11 02:02:20.553 : <epoch: 14, iter:  16,800, lr:1.000e-04> G_loss: 7.769e-02 
22-05-11 02:05:17.421 : <epoch: 14, iter:  17,000, lr:1.000e-04> G_loss: 2.359e-02 
22-05-11 02:08:14.091 : <epoch: 14, iter:  17,200, lr:1.000e-04> G_loss: 3.988e-02 
22-05-11 02:11:12.930 : <epoch: 15, iter:  17,400, lr:1.000e-04> G_loss: 3.519e-02 
22-05-11 02:14:09.940 : <epoch: 15, iter:  17,600, lr:1.000e-04> G_loss: 3.712e-02 
22-05-11 02:17:08.563 : <epoch: 15, iter:  17,800, lr:1.000e-04> G_loss: 3.568e-02 
22-05-11 02:20:07.143 : <epoch: 15, iter:  18,000, lr:1.000e-04> G_loss: 5.147e-02 
22-05-11 02:23:05.927 : <epoch: 15, iter:  18,200, lr:1.000e-04> G_loss: 4.345e-02 
22-05-11 02:26:03.890 : <epoch: 15, iter:  18,400, lr:1.000e-04> G_loss: 4.681e-02 
22-05-11 02:29:02.726 : <epoch: 16, iter:  18,600, lr:1.000e-04> G_loss: 4.955e-02 
22-05-11 02:32:00.830 : <epoch: 16, iter:  18,800, lr:1.000e-04> G_loss: 3.684e-02 
22-05-11 02:34:58.683 : <epoch: 16, iter:  19,000, lr:1.000e-04> G_loss: 1.822e-02 
22-05-11 02:37:56.260 : <epoch: 16, iter:  19,200, lr:1.000e-04> G_loss: 2.493e-02 
22-05-11 02:40:55.331 : <epoch: 16, iter:  19,400, lr:1.000e-04> G_loss: 4.806e-02 
22-05-11 02:43:55.802 : <epoch: 17, iter:  19,600, lr:1.000e-04> G_loss: 5.643e-02 
22-05-11 02:46:55.015 : <epoch: 17, iter:  19,800, lr:1.000e-04> G_loss: 4.946e-02 
22-05-11 02:49:53.439 : <epoch: 17, iter:  20,000, lr:1.000e-04> G_loss: 2.293e-02 
22-05-11 02:49:53.440 : Saving the model.
22-05-11 11:13:43.809 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: SR/usrnet_ST/models/20000_G.pth
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: SR/usrnet_ST/models/20000_E.pth
    pretrained_optimizerG: SR/usrnet_ST/models/20000_optimizerG.pth
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 20000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-05-11 11:13:46.362 : Number of train images: 3,450, iters: 1,150
22-05-11 11:13:57.915 : 
Networks name: USRNet_ST
Params number: 2204204
Net structure:
USRNet_ST(
  (d): DataNet()
  (p): ResUNet(
    (m_head): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (m_down1): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down2): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down3): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (sw_res1): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res2): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res3): SwinIR(
      (conv_first): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res4): SwinIR(
      (conv_first): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (m_up3): Sequential(
      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up2): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up1): Sequential(
      (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_tail): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (h): HyPaNet(
    (mlp): Sequential(
      (0): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (3): ReLU(inplace=True)
      (4): Conv2d(32, 12, kernel_size=(1, 1), stride=(1, 1))
      (5): Softplus(beta=1, threshold=20)
    )
  )
)

22-05-11 11:13:58.003 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.002 | -0.134 |  0.136 |  0.040 | torch.Size([16, 4, 3, 3]) || p.m_head.weight
 |  0.001 | -0.125 |  0.151 |  0.027 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.0.weight
 | -0.000 | -0.137 |  0.120 |  0.032 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.2.weight
 |  0.001 | -0.120 |  0.117 |  0.025 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.0.weight
 |  0.000 | -0.123 |  0.115 |  0.028 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.2.weight
 |  0.000 | -0.102 |  0.104 |  0.029 | torch.Size([32, 16, 2, 2]) || p.m_down1.2.weight
 |  0.000 | -0.115 |  0.106 |  0.019 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.0.weight
 |  0.000 | -0.142 |  0.150 |  0.020 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.2.weight
 | -0.000 | -0.130 |  0.094 |  0.019 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.0.weight
 | -0.000 | -0.140 |  0.122 |  0.020 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.2.weight
 | -0.000 | -0.107 |  0.106 |  0.024 | torch.Size([64, 32, 2, 2]) || p.m_down2.2.weight
 | -0.000 | -0.079 |  0.083 |  0.014 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.0.weight
 |  0.000 | -0.121 |  0.113 |  0.014 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.2.weight
 | -0.000 | -0.102 |  0.084 |  0.015 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.0.weight
 | -0.000 | -0.115 |  0.097 |  0.015 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.2.weight
 | -0.000 | -0.106 |  0.123 |  0.019 | torch.Size([64, 64, 2, 2]) || p.m_down3.2.weight
 |  0.000 | -0.034 |  0.037 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_first.bias
 |  1.006 |  0.973 |  1.066 |  0.019 | torch.Size([64]) || p.sw_res1.patch_embed.norm.weight
 |  0.000 | -0.032 |  0.027 |  0.011 | torch.Size([64]) || p.sw_res1.patch_embed.norm.bias
 |  0.994 |  0.979 |  1.023 |  0.009 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.024 |  0.021 |  0.012 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.bias
 |  0.004 | -0.091 |  0.086 |  0.032 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.077 |  0.082 |  0.019 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.037 |  0.035 |  0.009 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.116 |  0.087 |  0.026 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.001 | -0.018 |  0.016 |  0.009 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.bias
 |  0.999 |  0.964 |  1.034 |  0.014 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.weight
 | -0.001 | -0.032 |  0.017 |  0.011 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.080 |  0.091 |  0.021 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.011 |  0.014 |  0.005 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.076 |  0.077 |  0.021 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.001 | -0.018 |  0.016 |  0.009 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn_mask
 |  0.993 |  0.962 |  1.022 |  0.012 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.033 |  0.026 |  0.013 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.bias
 |  0.006 | -0.069 |  0.061 |  0.026 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.094 |  0.083 |  0.021 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.weight
 | -0.002 | -0.044 |  0.053 |  0.012 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.107 |  0.113 |  0.026 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.001 | -0.018 |  0.016 |  0.008 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.973 |  1.029 |  0.014 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.021 |  0.022 |  0.011 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.074 |  0.078 |  0.021 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.weight
 | -0.001 | -0.017 |  0.009 |  0.005 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.001 | -0.083 |  0.073 |  0.021 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.018 |  0.015 |  0.008 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  0.995 |  0.971 |  1.020 |  0.011 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.weight
 |  0.001 | -0.027 |  0.023 |  0.012 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.bias
 |  0.004 | -0.128 |  0.090 |  0.037 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.099 |  0.093 |  0.024 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.001 | -0.028 |  0.026 |  0.009 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.088 |  0.093 |  0.026 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.001 | -0.016 |  0.014 |  0.007 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.003 |  0.973 |  1.037 |  0.014 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.022 |  0.018 |  0.011 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.083 |  0.085 |  0.021 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.weight
 | -0.000 | -0.010 |  0.011 |  0.005 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.082 |  0.103 |  0.021 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.001 | -0.017 |  0.014 |  0.007 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  0.970 |  1.036 |  0.013 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.weight
 |  0.001 | -0.022 |  0.030 |  0.012 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.bias
 |  0.008 | -0.058 |  0.074 |  0.033 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.113 |  0.128 |  0.027 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.001 | -0.025 |  0.050 |  0.009 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.104 |  0.089 |  0.026 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.001 | -0.017 |  0.013 |  0.007 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.971 |  1.055 |  0.016 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.022 |  0.029 |  0.012 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.090 |  0.099 |  0.022 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.weight
 | -0.000 | -0.014 |  0.010 |  0.005 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.001 | -0.082 |  0.074 |  0.021 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.001 | -0.017 |  0.014 |  0.006 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.065 |  0.069 |  0.013 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.0.conv.weight
 |  0.000 | -0.029 |  0.025 |  0.010 | torch.Size([64]) || p.sw_res1.layers.0.conv.bias
 |  0.996 |  0.971 |  1.030 |  0.012 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.weight
 | -0.001 | -0.022 |  0.022 |  0.009 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.064 |  0.078 |  0.024 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.088 |  0.088 |  0.022 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.weight
 | -0.001 | -0.032 |  0.035 |  0.014 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.088 |  0.082 |  0.026 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.weight
 | -0.001 | -0.018 |  0.017 |  0.009 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  0.953 |  1.031 |  0.016 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.weight
 | -0.001 | -0.031 |  0.029 |  0.011 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.083 |  0.084 |  0.021 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.002 | -0.016 |  0.016 |  0.006 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.074 |  0.078 |  0.020 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.weight
 | -0.001 | -0.017 |  0.018 |  0.009 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn_mask
 |  0.995 |  0.967 |  1.026 |  0.013 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.weight
 | -0.001 | -0.024 |  0.021 |  0.010 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.bias
 |  0.002 | -0.047 |  0.052 |  0.023 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.068 |  0.076 |  0.019 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.weight
 | -0.002 | -0.034 |  0.033 |  0.011 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.096 |  0.092 |  0.026 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.weight
 | -0.001 | -0.021 |  0.021 |  0.010 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.003 |  0.961 |  1.029 |  0.015 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.022 |  0.024 |  0.010 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.081 |  0.079 |  0.021 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.002 | -0.017 |  0.016 |  0.006 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.087 |  0.092 |  0.020 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.weight
 | -0.001 | -0.022 |  0.017 |  0.008 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.967 |  1.042 |  0.014 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.weight
 | -0.001 | -0.023 |  0.016 |  0.009 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.bias
 |  0.003 | -0.062 |  0.051 |  0.025 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.104 |  0.112 |  0.025 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.001 | -0.061 |  0.061 |  0.020 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.087 |  0.086 |  0.026 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.weight
 | -0.001 | -0.020 |  0.022 |  0.011 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.002 |  0.965 |  1.044 |  0.016 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.028 |  0.032 |  0.010 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.076 |  0.085 |  0.021 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.020 |  0.015 |  0.006 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.068 |  0.074 |  0.020 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.weight
 | -0.001 | -0.023 |  0.022 |  0.010 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn_mask
 |  0.996 |  0.974 |  1.030 |  0.011 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.weight
 | -0.001 | -0.024 |  0.015 |  0.010 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.063 |  0.084 |  0.027 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.087 |  0.091 |  0.022 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.weight
 | -0.001 | -0.030 |  0.032 |  0.012 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.108 |  0.079 |  0.026 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.024 |  0.021 |  0.010 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.963 |  1.034 |  0.014 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.weight
 | -0.001 | -0.025 |  0.024 |  0.010 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.091 |  0.085 |  0.021 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.012 |  0.016 |  0.006 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.083 |  0.080 |  0.020 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.weight
 | -0.001 | -0.022 |  0.018 |  0.008 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.094 |  0.079 |  0.014 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.1.conv.weight
 |  0.001 | -0.021 |  0.030 |  0.013 | torch.Size([64]) || p.sw_res1.layers.1.conv.bias
 |  0.984 |  0.961 |  1.023 |  0.012 | torch.Size([64]) || p.sw_res1.norm.weight
 | -0.003 | -0.026 |  0.014 |  0.009 | torch.Size([64]) || p.sw_res1.norm.bias
 | -0.000 | -0.034 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_after_body.bias
 |  0.000 | -0.037 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_before_upsample.0.bias
 | -0.000 | -0.036 |  0.036 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res1.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res1.upsample.0.bias
 | -0.000 | -0.033 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_last.bias
 | -0.000 | -0.033 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_first.bias
 |  0.997 |  0.979 |  1.029 |  0.010 | torch.Size([64]) || p.sw_res2.patch_embed.norm.weight
 |  0.000 | -0.022 |  0.018 |  0.009 | torch.Size([64]) || p.sw_res2.patch_embed.norm.bias
 |  1.032 |  0.979 |  1.099 |  0.028 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.027 |  0.027 |  0.013 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.bias
 | -0.004 | -0.132 |  0.118 |  0.044 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.166 |  0.171 |  0.039 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.weight
 | -0.001 | -0.035 |  0.034 |  0.010 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.001 | -0.130 |  0.145 |  0.033 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.weight
 | -0.002 | -0.023 |  0.015 |  0.008 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.007 |  0.977 |  1.044 |  0.014 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.035 |  0.024 |  0.013 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.096 |  0.097 |  0.024 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.001 | -0.016 |  0.016 |  0.007 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.106 |  0.093 |  0.023 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.weight
 | -0.002 | -0.023 |  0.014 |  0.007 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn_mask
 |  1.032 |  0.984 |  1.095 |  0.023 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.028 |  0.043 |  0.011 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.bias
 | -0.012 | -0.094 |  0.132 |  0.044 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.159 |  0.160 |  0.040 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.020 |  0.022 |  0.007 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.127 |  0.122 |  0.034 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.weight
 | -0.002 | -0.015 |  0.014 |  0.007 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.009 |  0.976 |  1.046 |  0.015 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.weight
 | -0.001 | -0.033 |  0.030 |  0.014 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.113 |  0.082 |  0.024 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.weight
 | -0.001 | -0.028 |  0.016 |  0.008 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.086 |  0.100 |  0.022 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.weight
 | -0.002 | -0.015 |  0.014 |  0.007 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.027 |  0.978 |  1.088 |  0.026 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.weight
 | -0.001 | -0.021 |  0.020 |  0.010 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.bias
 | -0.008 | -0.096 |  0.155 |  0.039 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.155 |  0.153 |  0.039 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.001 | -0.027 |  0.024 |  0.009 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.001 | -0.121 |  0.113 |  0.036 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.weight
 | -0.002 | -0.016 |  0.015 |  0.007 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.015 |  0.969 |  1.061 |  0.019 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.weight
 | -0.001 | -0.025 |  0.029 |  0.013 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.124 |  0.098 |  0.024 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.020 |  0.018 |  0.008 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.087 |  0.100 |  0.022 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.weight
 | -0.001 | -0.019 |  0.013 |  0.007 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn_mask
 |  1.035 |  0.988 |  1.101 |  0.023 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.weight
 | -0.001 | -0.033 |  0.027 |  0.015 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.bias
 | -0.009 | -0.119 |  0.106 |  0.041 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.145 |  0.143 |  0.036 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.001 | -0.032 |  0.030 |  0.010 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.116 |  0.143 |  0.034 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.weight
 | -0.002 | -0.018 |  0.016 |  0.009 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.014 |  0.974 |  1.064 |  0.019 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.039 |  0.035 |  0.015 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.129 |  0.115 |  0.024 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.028 |  0.018 |  0.009 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.101 |  0.091 |  0.022 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.weight
 | -0.001 | -0.020 |  0.018 |  0.009 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.067 |  0.064 |  0.014 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.0.conv.weight
 |  0.001 | -0.035 |  0.031 |  0.013 | torch.Size([64]) || p.sw_res2.layers.0.conv.bias
 |  1.004 |  0.973 |  1.053 |  0.017 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.weight
 | -0.001 | -0.024 |  0.027 |  0.013 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.bias
 |  0.006 | -0.066 |  0.071 |  0.031 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.103 |  0.093 |  0.024 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.029 |  0.016 |  0.007 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.092 |  0.092 |  0.027 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.020 |  0.021 |  0.010 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.007 |  0.977 |  1.034 |  0.012 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.034 |  0.030 |  0.016 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.078 |  0.099 |  0.023 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.004 | -0.019 |  0.021 |  0.009 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.082 |  0.076 |  0.021 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.001 | -0.022 |  0.022 |  0.011 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn_mask
 |  1.005 |  0.969 |  1.045 |  0.016 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.weight
 | -0.001 | -0.031 |  0.040 |  0.016 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.bias
 |  0.006 | -0.105 |  0.070 |  0.034 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.092 |  0.107 |  0.023 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.001 | -0.022 |  0.020 |  0.008 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.107 |  0.105 |  0.027 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.018 |  0.020 |  0.010 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.006 |  0.973 |  1.044 |  0.013 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.weight
 | -0.001 | -0.030 |  0.031 |  0.016 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.092 |  0.094 |  0.022 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.003 | -0.018 |  0.023 |  0.009 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.087 |  0.082 |  0.021 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.018 |  0.024 |  0.010 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.002 |  0.964 |  1.028 |  0.014 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.weight
 |  0.001 | -0.040 |  0.037 |  0.016 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.bias
 |  0.007 | -0.077 |  0.085 |  0.033 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.092 |  0.082 |  0.020 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.025 |  0.023 |  0.008 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.109 |  0.089 |  0.027 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.017 |  0.018 |  0.009 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.005 |  0.962 |  1.044 |  0.013 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.weight
 | -0.002 | -0.038 |  0.042 |  0.018 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.bias
 | -0.001 | -0.113 |  0.092 |  0.022 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.004 | -0.024 |  0.025 |  0.011 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.074 |  0.086 |  0.021 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.019 |  0.019 |  0.010 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn_mask
 |  0.999 |  0.955 |  1.037 |  0.015 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.040 |  0.046 |  0.019 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.bias
 |  0.008 | -0.057 |  0.084 |  0.030 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.087 |  0.078 |  0.021 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.weight
 | -0.001 | -0.024 |  0.019 |  0.008 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.085 |  0.105 |  0.027 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.001 | -0.021 |  0.020 |  0.011 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.002 |  0.962 |  1.038 |  0.013 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.039 |  0.039 |  0.016 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.087 |  0.093 |  0.022 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.002 | -0.024 |  0.020 |  0.010 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.085 |  0.078 |  0.021 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.022 |  0.023 |  0.010 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.069 |  0.077 |  0.014 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.1.conv.weight
 |  0.000 | -0.029 |  0.023 |  0.012 | torch.Size([64]) || p.sw_res2.layers.1.conv.bias
 |  0.988 |  0.948 |  1.003 |  0.010 | torch.Size([64]) || p.sw_res2.norm.weight
 |  0.001 | -0.016 |  0.015 |  0.008 | torch.Size([64]) || p.sw_res2.norm.bias
 |  0.000 | -0.040 |  0.039 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_after_body.bias
 | -0.000 | -0.030 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_before_upsample.0.bias
 | -0.000 | -0.035 |  0.036 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res2.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res2.upsample.0.bias
 |  0.000 | -0.033 |  0.037 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_last.bias
 |  0.000 | -0.052 |  0.043 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_first.bias
 |  1.000 |  0.975 |  1.020 |  0.012 | torch.Size([32]) || p.sw_res3.patch_embed.norm.weight
 |  0.001 | -0.005 |  0.010 |  0.004 | torch.Size([32]) || p.sw_res3.patch_embed.norm.bias
 |  0.997 |  0.974 |  1.020 |  0.010 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.weight
 |  0.001 | -0.018 |  0.022 |  0.011 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.bias
 | -0.003 | -0.087 |  0.075 |  0.034 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.090 |  0.080 |  0.024 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.001 | -0.017 |  0.028 |  0.008 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.001 | -0.121 |  0.097 |  0.038 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.001 | -0.040 |  0.022 |  0.013 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.005 |  0.988 |  1.026 |  0.009 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.weight
 |  0.002 | -0.030 |  0.040 |  0.012 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.084 |  0.091 |  0.028 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.029 |  0.021 |  0.009 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.083 |  0.082 |  0.027 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.001 | -0.039 |  0.022 |  0.013 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn_mask
 |  1.004 |  0.977 |  1.030 |  0.012 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.011 |  0.017 |  0.008 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.092 |  0.096 |  0.036 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.089 |  0.094 |  0.026 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.027 |  0.021 |  0.008 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.112 |  0.137 |  0.039 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.001 | -0.037 |  0.022 |  0.012 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.003 |  0.988 |  1.024 |  0.011 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.015 |  0.026 |  0.010 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.084 |  0.090 |  0.027 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.001 | -0.020 |  0.020 |  0.009 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.078 |  0.084 |  0.027 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.001 | -0.037 |  0.023 |  0.013 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.971 |  1.023 |  0.012 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.014 |  0.013 |  0.008 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.bias
 | -0.003 | -0.072 |  0.087 |  0.032 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.089 |  0.096 |  0.027 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.001 | -0.025 |  0.022 |  0.007 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.001 | -0.100 |  0.114 |  0.038 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.001 | -0.036 |  0.020 |  0.012 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.005 |  0.983 |  1.026 |  0.011 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.weight
 |  0.002 | -0.010 |  0.013 |  0.007 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.bias
 | -0.001 | -0.091 |  0.088 |  0.028 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.weight
 | -0.001 | -0.019 |  0.019 |  0.008 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.001 | -0.093 |  0.091 |  0.028 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.001 | -0.036 |  0.022 |  0.012 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn_mask
 |  1.001 |  0.973 |  1.041 |  0.014 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.weight
 | -0.002 | -0.017 |  0.023 |  0.007 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.097 |  0.111 |  0.040 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.085 |  0.112 |  0.028 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.016 |  0.014 |  0.005 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.002 | -0.119 |  0.112 |  0.038 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.001 | -0.032 |  0.017 |  0.010 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.001 |  0.986 |  1.018 |  0.009 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.weight
 |  0.002 | -0.012 |  0.022 |  0.008 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.083 |  0.085 |  0.027 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.weight
 | -0.002 | -0.021 |  0.014 |  0.008 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.001 | -0.102 |  0.091 |  0.027 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.001 | -0.031 |  0.021 |  0.011 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.064 |  0.067 |  0.016 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.0.conv.weight
 |  0.000 | -0.010 |  0.011 |  0.006 | torch.Size([32]) || p.sw_res3.layers.0.conv.bias
 |  0.996 |  0.965 |  1.017 |  0.012 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.weight
 |  0.003 | -0.017 |  0.019 |  0.010 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.093 |  0.070 |  0.032 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.079 |  0.082 |  0.024 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.015 |  0.022 |  0.007 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.120 |  0.122 |  0.037 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.001 | -0.021 |  0.019 |  0.011 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.bias
 |  0.999 |  0.974 |  1.019 |  0.011 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.023 |  0.017 |  0.010 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.100 |  0.085 |  0.027 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.weight
 | -0.000 | -0.021 |  0.017 |  0.008 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.078 |  0.107 |  0.027 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.001 | -0.018 |  0.019 |  0.010 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn_mask
 |  1.002 |  0.977 |  1.039 |  0.013 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.weight
 | -0.001 | -0.019 |  0.011 |  0.007 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.110 |  0.118 |  0.049 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.079 |  0.112 |  0.025 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.016 |  0.023 |  0.007 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.124 |  0.104 |  0.038 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.001 | -0.019 |  0.018 |  0.010 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  0.976 |  1.022 |  0.012 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.weight
 | -0.001 | -0.023 |  0.012 |  0.009 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.084 |  0.087 |  0.027 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.weight
 | -0.001 | -0.019 |  0.012 |  0.008 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.084 |  0.098 |  0.028 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.001 | -0.020 |  0.018 |  0.011 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.969 |  1.020 |  0.010 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.weight
 |  0.002 | -0.014 |  0.018 |  0.009 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.099 |  0.089 |  0.036 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.086 |  0.095 |  0.025 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.weight
 | -0.001 | -0.019 |  0.019 |  0.006 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.105 |  0.110 |  0.037 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.001 | -0.018 |  0.018 |  0.010 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.bias
 |  0.997 |  0.978 |  1.022 |  0.010 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.weight
 |  0.002 | -0.014 |  0.021 |  0.008 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.078 |  0.100 |  0.027 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.weight
 | -0.003 | -0.022 |  0.019 |  0.009 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.001 | -0.094 |  0.082 |  0.027 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.001 | -0.023 |  0.019 |  0.010 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn_mask
 |  1.002 |  0.969 |  1.020 |  0.010 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.weight
 |  0.002 | -0.013 |  0.016 |  0.007 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.bias
 | -0.003 | -0.090 |  0.097 |  0.034 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.089 |  0.097 |  0.026 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.013 |  0.022 |  0.006 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.001 | -0.092 |  0.130 |  0.038 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.001 | -0.019 |  0.018 |  0.010 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.001 |  0.987 |  1.018 |  0.008 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.016 |  0.015 |  0.008 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.085 |  0.097 |  0.027 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.weight
 | -0.001 | -0.025 |  0.018 |  0.009 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.083 |  0.097 |  0.027 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.001 | -0.024 |  0.019 |  0.011 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.065 |  0.062 |  0.016 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.1.conv.weight
 | -0.001 | -0.016 |  0.015 |  0.010 | torch.Size([32]) || p.sw_res3.layers.1.conv.bias
 |  0.990 |  0.968 |  1.019 |  0.011 | torch.Size([32]) || p.sw_res3.norm.weight
 | -0.000 | -0.018 |  0.016 |  0.011 | torch.Size([32]) || p.sw_res3.norm.bias
 |  0.000 | -0.047 |  0.048 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_after_body.bias
 | -0.000 | -0.045 |  0.045 |  0.012 | torch.Size([64, 32, 3, 3]) || p.sw_res3.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.conv_before_upsample.0.bias
 | -0.000 | -0.036 |  0.034 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res3.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res3.upsample.0.bias
 | -0.000 | -0.033 |  0.033 |  0.008 | torch.Size([32, 64, 3, 3]) || p.sw_res3.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_last.bias
 | -0.000 | -0.053 |  0.053 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_first.bias
 |  1.002 |  0.987 |  1.017 |  0.008 | torch.Size([16]) || p.sw_res4.patch_embed.norm.weight
 |  0.002 | -0.011 |  0.012 |  0.008 | torch.Size([16]) || p.sw_res4.patch_embed.norm.bias
 |  0.994 |  0.977 |  1.012 |  0.011 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.weight
 |  0.003 | -0.009 |  0.019 |  0.008 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.bias
 | -0.003 | -0.136 |  0.149 |  0.046 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.094 |  0.097 |  0.033 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.019 |  0.016 |  0.007 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.002 | -0.139 |  0.111 |  0.051 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.weight
 | -0.001 | -0.018 |  0.011 |  0.009 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.988 |  1.018 |  0.009 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.weight
 |  0.002 | -0.009 |  0.019 |  0.007 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.105 |  0.099 |  0.038 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.weight
 | -0.002 | -0.015 |  0.015 |  0.008 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.001 | -0.119 |  0.127 |  0.038 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.weight
 | -0.001 | -0.017 |  0.010 |  0.009 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn_mask
 |  0.996 |  0.982 |  1.007 |  0.008 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.010 |  0.017 |  0.007 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.105 |  0.117 |  0.039 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.002 | -0.127 |  0.099 |  0.035 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.018 |  0.017 |  0.008 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.008 | -0.121 |  0.118 |  0.049 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.weight
 | -0.001 | -0.016 |  0.012 |  0.008 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.bias
 |  0.999 |  0.977 |  1.034 |  0.013 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.weight
 | -0.001 | -0.015 |  0.016 |  0.008 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.bias
 | -0.002 | -0.107 |  0.093 |  0.037 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.weight
 | -0.002 | -0.019 |  0.008 |  0.007 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.102 |  0.126 |  0.038 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.weight
 | -0.001 | -0.015 |  0.010 |  0.007 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.983 |  1.030 |  0.012 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.weight
 |  0.002 | -0.007 |  0.012 |  0.006 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.bias
 | -0.007 | -0.184 |  0.132 |  0.059 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.111 |  0.080 |  0.033 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.002 | -0.009 |  0.021 |  0.006 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.002 | -0.114 |  0.130 |  0.051 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.weight
 | -0.001 | -0.013 |  0.011 |  0.007 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.bias
 |  0.997 |  0.975 |  1.030 |  0.013 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.weight
 |  0.003 | -0.011 |  0.033 |  0.011 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.bias
 |  0.002 | -0.110 |  0.131 |  0.037 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.weight
 | -0.002 | -0.018 |  0.013 |  0.007 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.001 | -0.125 |  0.095 |  0.037 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.weight
 | -0.001 | -0.014 |  0.012 |  0.008 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn_mask
 |  0.997 |  0.970 |  1.013 |  0.011 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.weight
 | -0.001 | -0.012 |  0.015 |  0.008 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.bias
 | -0.004 | -0.234 |  0.176 |  0.054 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.002 | -0.091 |  0.117 |  0.034 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.011 |  0.017 |  0.006 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.007 | -0.136 |  0.121 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.014 |  0.014 |  0.007 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.bias
 |  0.996 |  0.987 |  1.009 |  0.007 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 | -0.012 |  0.011 |  0.006 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.112 |  0.128 |  0.037 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.weight
 | -0.004 | -0.016 |  0.007 |  0.006 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.002 | -0.101 |  0.099 |  0.037 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.weight
 | -0.001 | -0.012 |  0.012 |  0.007 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.065 |  0.069 |  0.020 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.0.conv.weight
 |  0.000 | -0.014 |  0.012 |  0.010 | torch.Size([16]) || p.sw_res4.layers.0.conv.bias
 |  0.995 |  0.962 |  1.012 |  0.013 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.weight
 | -0.001 | -0.022 |  0.012 |  0.008 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.bias
 | -0.010 | -0.247 |  0.122 |  0.064 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.106 |  0.116 |  0.032 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.001 | -0.017 |  0.013 |  0.006 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.116 |  0.160 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.weight
 | -0.002 | -0.014 |  0.009 |  0.007 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.bias
 |  0.998 |  0.977 |  1.020 |  0.013 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.weight
 | -0.002 | -0.011 |  0.011 |  0.006 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.112 |  0.108 |  0.037 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.weight
 | -0.004 | -0.017 |  0.005 |  0.006 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.125 |  0.108 |  0.038 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.weight
 | -0.001 | -0.010 |  0.010 |  0.006 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn_mask
 |  0.995 |  0.970 |  1.020 |  0.011 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.weight
 | -0.001 | -0.013 |  0.015 |  0.007 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.129 |  0.150 |  0.051 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.001 | -0.113 |  0.088 |  0.033 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.001 | -0.011 |  0.013 |  0.005 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.123 |  0.115 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.weight
 | -0.002 | -0.013 |  0.010 |  0.006 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.bias
 |  0.996 |  0.966 |  1.018 |  0.012 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.weight
 | -0.001 | -0.021 |  0.008 |  0.007 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.bias
 | -0.002 | -0.115 |  0.130 |  0.037 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.weight
 | -0.004 | -0.016 |  0.011 |  0.006 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.108 |  0.118 |  0.037 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.weight
 | -0.002 | -0.011 |  0.012 |  0.006 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  0.993 |  0.963 |  1.010 |  0.012 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.weight
 |  0.003 | -0.009 |  0.012 |  0.006 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.bias
 |  0.002 | -0.160 |  0.168 |  0.056 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.003 | -0.132 |  0.085 |  0.034 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.029 |  0.028 |  0.009 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.107 |  0.126 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.weight
 | -0.002 | -0.014 |  0.010 |  0.007 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.bias
 |  0.996 |  0.975 |  1.021 |  0.012 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.009 |  0.008 |  0.005 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.bias
 |  0.001 | -0.115 |  0.093 |  0.036 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.weight
 | -0.004 | -0.018 |  0.006 |  0.005 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.001 | -0.110 |  0.100 |  0.037 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.weight
 | -0.001 | -0.011 |  0.009 |  0.006 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn_mask
 |  0.985 |  0.954 |  1.009 |  0.017 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 | -0.013 |  0.010 |  0.006 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.bias
 |  0.004 | -0.136 |  0.202 |  0.053 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.001 | -0.125 |  0.103 |  0.033 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.017 |  0.011 |  0.006 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.003 | -0.144 |  0.123 |  0.049 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.weight
 | -0.002 | -0.011 |  0.012 |  0.006 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.bias
 |  0.991 |  0.954 |  1.007 |  0.013 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.weight
 | -0.002 | -0.014 |  0.006 |  0.006 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.bias
 | -0.001 | -0.126 |  0.099 |  0.036 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.weight
 | -0.003 | -0.015 |  0.008 |  0.005 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.001 | -0.110 |  0.102 |  0.037 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.weight
 | -0.002 | -0.012 |  0.011 |  0.006 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.074 |  0.067 |  0.020 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.1.conv.weight
 |  0.000 | -0.021 |  0.015 |  0.012 | torch.Size([16]) || p.sw_res4.layers.1.conv.bias
 |  0.976 |  0.940 |  0.993 |  0.015 | torch.Size([16]) || p.sw_res4.norm.weight
 | -0.000 | -0.010 |  0.011 |  0.008 | torch.Size([16]) || p.sw_res4.norm.bias
 | -0.000 | -0.060 |  0.057 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_after_body.bias
 | -0.000 | -0.063 |  0.061 |  0.017 | torch.Size([64, 16, 3, 3]) || p.sw_res4.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res4.conv_before_upsample.0.bias
 | -0.000 | -0.034 |  0.035 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res4.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res4.upsample.0.bias
 | -0.000 | -0.033 |  0.028 |  0.008 | torch.Size([16, 64, 3, 3]) || p.sw_res4.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_last.bias
 | -0.000 | -0.081 |  0.072 |  0.017 | torch.Size([64, 64, 2, 2]) || p.m_up3.0.weight
 |  0.000 | -0.104 |  0.095 |  0.017 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.0.weight
 |  0.000 | -0.094 |  0.084 |  0.016 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.2.weight
 | -0.000 | -0.114 |  0.105 |  0.018 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.0.weight
 | -0.000 | -0.111 |  0.099 |  0.018 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.2.weight
 |  0.000 | -0.070 |  0.101 |  0.020 | torch.Size([64, 32, 2, 2]) || p.m_up2.0.weight
 | -0.000 | -0.069 |  0.079 |  0.017 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.0.weight
 | -0.001 | -0.086 |  0.085 |  0.018 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.2.weight
 | -0.000 | -0.074 |  0.079 |  0.017 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.0.weight
 | -0.000 | -0.097 |  0.125 |  0.018 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.2.weight
 | -0.001 | -0.094 |  0.097 |  0.026 | torch.Size([32, 16, 2, 2]) || p.m_up1.0.weight
 |  0.001 | -0.067 |  0.058 |  0.019 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.0.weight
 |  0.001 | -0.063 |  0.065 |  0.018 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.2.weight
 |  0.000 | -0.075 |  0.059 |  0.019 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.0.weight
 |  0.001 | -0.055 |  0.052 |  0.018 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.2.weight
 | -0.001 | -0.065 |  0.062 |  0.023 | torch.Size([3, 16, 3, 3]) || p.m_tail.weight
 | -0.125 | -0.667 |  0.150 |  0.254 | torch.Size([32, 2, 1, 1]) || h.mlp.0.weight
 |  0.096 | -0.003 |  0.267 |  0.108 | torch.Size([32]) || h.mlp.0.bias
 |  0.021 | -0.095 |  0.303 |  0.061 | torch.Size([32, 32, 1, 1]) || h.mlp.2.weight
 |  0.052 | -0.006 |  0.179 |  0.063 | torch.Size([32]) || h.mlp.2.bias
 | -0.084 | -0.645 |  0.129 |  0.167 | torch.Size([12, 32, 1, 1]) || h.mlp.4.weight
 | -0.140 | -0.382 |  0.022 |  0.161 | torch.Size([12]) || h.mlp.4.bias

