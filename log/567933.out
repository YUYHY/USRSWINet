export CUDA_VISIBLE_DEVICES=0
number of GPUs is: 1
LogHandlers setup!
22-05-10 21:55:06.002 :   task: usrnet_ST
  model: plain4
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  merge_bn: False
  merge_bn_startpoint: 300000
  datasets:[
    train:[
      name: train_dataset
      dataset_type: usrnet
      dataroot_H: ['/scratch_net/ken/jiezcao/data/Flickr2K/Flickr2K_HR', '/scratch_net/ken/jiezcao/data/DIV2K/DIV2K_train_HR']
      dataroot_L: None
      H_size: 96
      use_flip: True
      use_rot: True
      scales: [1, 2, 3, 4]
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 3
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: usrnet
      dataroot_H: /scratch_net/ken/jiezcao/data/CBSD68/original_png
      dataroot_L: None
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  path:[
    root: SR
    pretrained_netG: None
    task: SR/usrnet_ST
    log: SR/usrnet_ST
    options: SR/usrnet_ST/options
    models: SR/usrnet_ST/models
    images: SR/usrnet_ST/images
    pretrained_netE: None
    pretrained_optimizerG: None
  ]
  netG:[
    net_type: usrnet_ST
    n_iter: 6
    h_nc: 32
    in_nc: 4
    out_nc: 3
    nc: [16, 32, 64, 64]
    nb: 2
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    resi_connection: 1conv
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [100000, 200000, 300000, 400000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 20000
    checkpoint_save: 5000
    checkpoint_print: 200
    E_decay: 0.999
    G_optimizer_reuse: True
    G_param_strict: True
    E_param_strict: True
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options/train_usrnet_ST.json
  is_train: True
  find_unused_parameters: True
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

Random seed: 3734
Dataset [DatasetUSRNet - train_dataset] is created.
22-05-10 21:55:08.406 : Number of train images: 3,450, iters: 1,150
/scratch_net/ken/jiezcao/anaconda3/envs/usrswin/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Dataset [DatasetUSRNet - test_dataset] is created.
train_loader length  1150
test_loader length  68
Initialization method [orthogonal + uniform], gain is [0.20]
Initialization method [orthogonal + uniform], gain is [0.20]
Training model [ModelPlain4] is created.
Copying model for E ...
22-05-10 21:55:19.085 : 
Networks name: USRNet_ST
Params number: 2204204
Net structure:
USRNet_ST(
  (d): DataNet()
  (p): ResUNet(
    (m_head): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (m_down1): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down2): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (m_down3): Sequential(
      (0): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (sw_res1): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(12, 12), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(12, 12), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res2): SwinIR(
      (conv_first): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=64, input_resolution=(24, 24), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=64, input_resolution=(24, 24), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=64, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=64, out_features=192, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=64, out_features=64, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=64, out_features=128, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=128, out_features=64, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res3): SwinIR(
      (conv_first): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=32, input_resolution=(48, 48), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=32, input_resolution=(48, 48), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=32, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=32, out_features=96, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=32, out_features=32, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=32, out_features=64, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=64, out_features=32, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sw_res4): SwinIR(
      (conv_first): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=16, input_resolution=(96, 96), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=0, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=16, input_resolution=(96, 96), num_heads=4, window_size=3, shift_size=1, mlp_ratio=2
                (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=16, window_size=(3, 3), num_heads=4
                  (qkv): Linear(in_features=16, out_features=48, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=16, out_features=16, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath()
                (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=16, out_features=32, bias=True)
                  (act): GELU()
                  (fc2): Linear(in_features=32, out_features=16, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
        )
      )
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_before_upsample): Sequential(
        (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (upsample): Upsample(
        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): PixelShuffle(upscale_factor=2)
      )
      (conv_last): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (m_up3): Sequential(
      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up2): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_up1): Sequential(
      (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (2): ResBlock(
        (res): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (m_tail): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (h): HyPaNet(
    (mlp): Sequential(
      (0): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (3): ReLU(inplace=True)
      (4): Conv2d(32, 12, kernel_size=(1, 1), stride=(1, 1))
      (5): Softplus(beta=1, threshold=20)
    )
  )
)

22-05-10 21:55:19.172 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.001 | -0.113 |  0.090 |  0.033 | torch.Size([16, 4, 3, 3]) || p.m_head.weight
 | -0.001 | -0.068 |  0.062 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.0.weight
 |  0.000 | -0.057 |  0.052 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.0.res.2.weight
 | -0.000 | -0.056 |  0.057 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.0.weight
 |  0.001 | -0.066 |  0.061 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_down1.1.res.2.weight
 |  0.000 | -0.088 |  0.091 |  0.025 | torch.Size([32, 16, 2, 2]) || p.m_down1.2.weight
 |  0.000 | -0.042 |  0.044 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.0.weight
 |  0.000 | -0.042 |  0.044 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.0.res.2.weight
 | -0.000 | -0.046 |  0.040 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.0.weight
 | -0.000 | -0.042 |  0.046 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_down2.1.res.2.weight
 | -0.000 | -0.060 |  0.058 |  0.018 | torch.Size([64, 32, 2, 2]) || p.m_down2.2.weight
 | -0.000 | -0.037 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.0.weight
 | -0.000 | -0.036 |  0.030 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.0.res.2.weight
 |  0.000 | -0.035 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.0.weight
 |  0.000 | -0.034 |  0.040 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_down3.1.res.2.weight
 | -0.000 | -0.048 |  0.052 |  0.012 | torch.Size([64, 64, 2, 2]) || p.m_down3.2.weight
 |  0.000 | -0.034 |  0.037 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.046 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.051 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.107 |  0.088 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.064 |  0.062 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.065 |  0.067 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm1.bias
 |  0.003 | -0.037 |  0.045 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.053 |  0.052 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.080 |  0.100 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.063 |  0.061 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.073 |  0.070 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.053 |  0.051 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.055 |  0.054 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.080 |  0.079 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.064 |  0.063 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.062 |  0.075 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm1.bias
 |  0.003 | -0.051 |  0.051 |  0.017 | torch.Size([25, 4]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.051 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.090 |  0.083 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.061 |  0.059 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.061 |  0.061 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.034 |  0.036 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.052 |  0.059 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.054 |  0.060 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.001 | -0.078 |  0.081 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.063 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.059 |  0.074 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.043 |  0.043 |  0.019 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.053 |  0.061 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.085 |  0.091 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.058 |  0.070 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.077 |  0.066 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.054 |  0.053 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.051 |  0.055 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.092 |  0.087 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.071 |  0.065 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.067 |  0.068 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -20.988 | -100.000 |  0.000 | 40.738 | torch.Size([16, 9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm1.bias
 | -0.002 | -0.049 |  0.076 |  0.020 | torch.Size([25, 4]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.054 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.105 |  0.079 |  0.025 | torch.Size([64, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.067 |  0.059 |  0.018 | torch.Size([128, 64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.062 |  0.062 |  0.018 | torch.Size([64, 128]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.032 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res1.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.norm.bias
 | -0.000 | -0.034 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_after_body.bias
 |  0.000 | -0.037 |  0.035 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_before_upsample.0.bias
 | -0.000 | -0.036 |  0.036 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res1.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res1.upsample.0.bias
 | -0.000 | -0.033 |  0.033 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res1.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res1.conv_last.bias
 | -0.000 | -0.033 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm1.bias
 |  0.002 | -0.066 |  0.047 |  0.018 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.063 |  0.051 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.075 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.069 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.066 |  0.074 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm1.bias
 | -0.006 | -0.062 |  0.046 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.056 |  0.058 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.084 |  0.078 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.062 |  0.058 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.071 |  0.059 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.043 |  0.045 |  0.017 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.058 |  0.054 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.079 |  0.083 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.059 |  0.065 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.064 |  0.062 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.051 |  0.046 |  0.021 | torch.Size([25, 4]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.059 |  0.076 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.078 |  0.098 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.066 |  0.060 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.061 |  0.067 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.031 |  0.036 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.047 |  0.056 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.052 |  0.061 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.084 |  0.088 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.060 |  0.069 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.066 |  0.063 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm1.bias
 | -0.002 | -0.036 |  0.050 |  0.019 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.053 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.090 |  0.098 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.063 |  0.082 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.068 |  0.061 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.053 |  0.046 |  0.023 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.061 |  0.056 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.080 |  0.082 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.062 |  0.069 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.066 |  0.060 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -10.802 | -100.000 |  0.000 | 31.044 | torch.Size([64, 9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.038 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.059 |  0.057 |  0.014 | torch.Size([192, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([192]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.083 |  0.086 |  0.025 | torch.Size([64, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.062 |  0.069 |  0.018 | torch.Size([128, 64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.062 |  0.065 |  0.018 | torch.Size([64, 128]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.033 |  0.040 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || p.sw_res2.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.norm.bias
 |  0.000 | -0.040 |  0.039 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_after_body.bias
 | -0.000 | -0.030 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_before_upsample.0.bias
 | -0.000 | -0.035 |  0.036 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res2.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res2.upsample.0.bias
 |  0.000 | -0.033 |  0.037 |  0.008 | torch.Size([64, 64, 3, 3]) || p.sw_res2.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res2.conv_last.bias
 |  0.000 | -0.052 |  0.043 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm1.bias
 | -0.002 | -0.044 |  0.062 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.078 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.001 | -0.096 |  0.102 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.079 |  0.089 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.080 |  0.083 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.067 |  0.045 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.084 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.106 |  0.127 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.077 |  0.080 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.070 |  0.074 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.041 |  0.055 |  0.019 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.066 |  0.076 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.001 | -0.097 |  0.096 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.norm2.bias
 | -0.001 | -0.076 |  0.075 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.001 | -0.075 |  0.091 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.046 |  0.065 |  0.022 | torch.Size([25, 4]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.065 |  0.067 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.113 |  0.112 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.076 |  0.079 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.001 | -0.081 |  0.076 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.045 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm1.bias
 |  0.002 | -0.051 |  0.059 |  0.020 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.061 |  0.080 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.110 |  0.118 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.norm2.bias
 | -0.001 | -0.079 |  0.083 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.077 |  0.099 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.051 |  0.051 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.068 |  0.067 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.119 |  0.093 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.074 |  0.080 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.083 |  0.087 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.052 |  0.061 |  0.021 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.070 |  0.069 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.093 |  0.112 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.072 |  0.094 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.094 |  0.078 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -5.478 | -100.000 |  0.000 | 22.756 | torch.Size([256, 9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm1.bias
 | -0.002 | -0.054 |  0.043 |  0.017 | torch.Size([25, 4]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.071 |  0.070 |  0.020 | torch.Size([96, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([96]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.001 | -0.096 |  0.107 |  0.035 | torch.Size([32, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.077 |  0.087 |  0.025 | torch.Size([64, 32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.079 |  0.081 |  0.025 | torch.Size([32, 64]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.040 |  0.043 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([32]) || p.sw_res3.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.norm.bias
 |  0.000 | -0.047 |  0.048 |  0.012 | torch.Size([32, 32, 3, 3]) || p.sw_res3.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_after_body.bias
 | -0.000 | -0.045 |  0.045 |  0.012 | torch.Size([64, 32, 3, 3]) || p.sw_res3.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res3.conv_before_upsample.0.bias
 | -0.000 | -0.036 |  0.034 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res3.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res3.upsample.0.bias
 | -0.000 | -0.033 |  0.033 |  0.008 | torch.Size([32, 64, 3, 3]) || p.sw_res3.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res3.conv_last.bias
 | -0.000 | -0.053 |  0.053 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_first.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.056 |  0.056 |  0.023 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.087 |  0.090 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.003 | -0.110 |  0.123 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.106 |  0.090 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.107 |  0.113 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm1.bias
 |  0.002 | -0.034 |  0.047 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.001 | -0.088 |  0.082 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.007 | -0.130 |  0.120 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.norm2.bias
 | -0.001 | -0.091 |  0.093 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.089 |  0.102 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.047 |  0.046 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.092 |  0.075 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.002 | -0.114 |  0.129 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.norm2.bias
 |  0.002 | -0.098 |  0.104 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.001 | -0.101 |  0.098 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.045 |  0.049 |  0.019 | torch.Size([25, 4]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.002 | -0.076 |  0.074 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.008 | -0.122 |  0.111 |  0.049 | torch.Size([16, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.122 |  0.113 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.002 | -0.109 |  0.092 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.052 |  0.052 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.0.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm1.bias
 | -0.003 | -0.043 |  0.042 |  0.017 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.086 |  0.095 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.001 | -0.129 |  0.136 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.norm2.bias
 |  0.001 | -0.100 |  0.100 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.102 |  0.101 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.050 |  0.057 |  0.021 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.104 |  0.098 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.129 |  0.125 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.norm2.bias
 | -0.002 | -0.117 |  0.098 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.109 |  0.104 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm1.bias
 |  0.002 | -0.041 |  0.055 |  0.018 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.003 | -0.086 |  0.088 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.120 |  0.131 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.norm2.bias
 |  0.001 | -0.103 |  0.097 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.001 | -0.100 |  0.091 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -2.758 | -100.000 |  0.000 | 16.378 | torch.Size([1024, 9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.062 |  0.054 |  0.020 | torch.Size([25, 4]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 12.000 |  0.000 | 24.000 |  5.925 | torch.Size([9, 9]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.001 | -0.093 |  0.090 |  0.029 | torch.Size([48, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([48]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.003 | -0.127 |  0.115 |  0.050 | torch.Size([16, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.norm2.bias
 | -0.001 | -0.136 |  0.095 |  0.035 | torch.Size([32, 16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.088 |  0.090 |  0.035 | torch.Size([16, 32]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.055 |  0.054 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.layers.1.conv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([16]) || p.sw_res4.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.norm.bias
 | -0.000 | -0.060 |  0.057 |  0.017 | torch.Size([16, 16, 3, 3]) || p.sw_res4.conv_after_body.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_after_body.bias
 | -0.000 | -0.063 |  0.061 |  0.017 | torch.Size([64, 16, 3, 3]) || p.sw_res4.conv_before_upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || p.sw_res4.conv_before_upsample.0.bias
 | -0.000 | -0.034 |  0.035 |  0.008 | torch.Size([256, 64, 3, 3]) || p.sw_res4.upsample.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([256]) || p.sw_res4.upsample.0.bias
 | -0.000 | -0.033 |  0.028 |  0.008 | torch.Size([16, 64, 3, 3]) || p.sw_res4.conv_last.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([16]) || p.sw_res4.conv_last.bias
 | -0.000 | -0.044 |  0.052 |  0.013 | torch.Size([64, 64, 2, 2]) || p.m_up3.0.weight
 |  0.000 | -0.036 |  0.031 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.0.weight
 |  0.000 | -0.031 |  0.032 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.1.res.2.weight
 | -0.000 | -0.031 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.0.weight
 | -0.000 | -0.036 |  0.034 |  0.008 | torch.Size([64, 64, 3, 3]) || p.m_up3.2.res.2.weight
 |  0.000 | -0.061 |  0.068 |  0.018 | torch.Size([64, 32, 2, 2]) || p.m_up2.0.weight
 | -0.000 | -0.041 |  0.046 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.0.weight
 | -0.000 | -0.044 |  0.048 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.1.res.2.weight
 |  0.000 | -0.044 |  0.041 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.0.weight
 | -0.000 | -0.048 |  0.045 |  0.012 | torch.Size([32, 32, 3, 3]) || p.m_up2.2.res.2.weight
 | -0.001 | -0.089 |  0.076 |  0.025 | torch.Size([32, 16, 2, 2]) || p.m_up1.0.weight
 |  0.000 | -0.059 |  0.061 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.0.weight
 |  0.000 | -0.062 |  0.060 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.1.res.2.weight
 |  0.000 | -0.056 |  0.048 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.0.weight
 |  0.000 | -0.054 |  0.050 |  0.017 | torch.Size([16, 16, 3, 3]) || p.m_up1.2.res.2.weight
 | -0.000 | -0.051 |  0.048 |  0.017 | torch.Size([3, 16, 3, 3]) || p.m_tail.weight
 |  0.004 | -0.084 |  0.099 |  0.035 | torch.Size([32, 2, 1, 1]) || h.mlp.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || h.mlp.0.bias
 | -0.000 | -0.095 |  0.107 |  0.035 | torch.Size([32, 32, 1, 1]) || h.mlp.2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([32]) || h.mlp.2.bias
 | -0.001 | -0.115 |  0.094 |  0.035 | torch.Size([12, 32, 1, 1]) || h.mlp.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([12]) || h.mlp.4.bias

/scratch_net/ken/jiezcao/anaconda3/envs/usrswin/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/scratch_net/ken/jiezcao/anaconda3/envs/usrswin/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[W Copy.cpp:219] Warning: Casting complex values to real discards the imaginary part (function operator())
/scratch_net/ken/jiezcao/anaconda3/envs/usrswin/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  "please use `get_last_lr()`.", UserWarning)
22-05-10 21:58:16.051 : <epoch:  0, iter:     200, lr:1.000e-04> G_loss: 2.120e-01 
22-05-10 22:01:09.922 : <epoch:  0, iter:     400, lr:1.000e-04> G_loss: 1.862e-01 
22-05-10 22:04:03.844 : <epoch:  0, iter:     600, lr:1.000e-04> G_loss: 1.394e-01 
22-05-10 22:06:57.884 : <epoch:  0, iter:     800, lr:1.000e-04> G_loss: 1.737e-01 
22-05-10 22:09:51.544 : <epoch:  0, iter:   1,000, lr:1.000e-04> G_loss: 1.462e-01 
22-05-10 22:12:49.696 : <epoch:  1, iter:   1,200, lr:1.000e-04> G_loss: 2.735e-01 
22-05-10 22:15:44.361 : <epoch:  1, iter:   1,400, lr:1.000e-04> G_loss: 1.994e-01 
22-05-10 22:18:38.638 : <epoch:  1, iter:   1,600, lr:1.000e-04> G_loss: 2.140e-01 
22-05-10 22:21:32.239 : <epoch:  1, iter:   1,800, lr:1.000e-04> G_loss: 1.972e-01 
22-05-10 22:24:25.988 : <epoch:  1, iter:   2,000, lr:1.000e-04> G_loss: 6.149e-02 
22-05-10 22:27:19.926 : <epoch:  1, iter:   2,200, lr:1.000e-04> G_loss: 7.116e-02 
22-05-10 22:30:15.593 : <epoch:  2, iter:   2,400, lr:1.000e-04> G_loss: 1.373e-01 
22-05-10 22:33:09.367 : <epoch:  2, iter:   2,600, lr:1.000e-04> G_loss: 1.300e-01 
22-05-10 22:36:03.316 : <epoch:  2, iter:   2,800, lr:1.000e-04> G_loss: 7.682e-02 
22-05-10 22:38:57.350 : <epoch:  2, iter:   3,000, lr:1.000e-04> G_loss: 7.946e-02 
22-05-10 22:41:51.469 : <epoch:  2, iter:   3,200, lr:1.000e-04> G_loss: 8.176e-02 
22-05-10 22:44:45.548 : <epoch:  2, iter:   3,400, lr:1.000e-04> G_loss: 5.347e-02 
22-05-10 22:47:42.113 : <epoch:  3, iter:   3,600, lr:1.000e-04> G_loss: 5.439e-02 
22-05-10 22:50:36.724 : <epoch:  3, iter:   3,800, lr:1.000e-04> G_loss: 5.406e-02 
22-05-10 22:53:32.228 : <epoch:  3, iter:   4,000, lr:1.000e-04> G_loss: 4.476e-02 
22-05-10 22:56:27.595 : <epoch:  3, iter:   4,200, lr:1.000e-04> G_loss: 6.061e-02 
22-05-10 22:59:22.540 : <epoch:  3, iter:   4,400, lr:1.000e-04> G_loss: 4.919e-02 
22-05-10 23:02:17.393 : <epoch:  3, iter:   4,600, lr:1.000e-04> G_loss: 6.682e-02 
22-05-10 23:05:14.527 : <epoch:  4, iter:   4,800, lr:1.000e-04> G_loss: 7.343e-02 
22-05-10 23:08:09.357 : <epoch:  4, iter:   5,000, lr:1.000e-04> G_loss: 3.755e-02 
22-05-10 23:08:09.358 : Saving the model.
22-05-10 23:11:06.297 : <epoch:  4, iter:   5,200, lr:1.000e-04> G_loss: 5.332e-02 
22-05-10 23:14:01.616 : <epoch:  4, iter:   5,400, lr:1.000e-04> G_loss: 4.115e-02 
22-05-10 23:16:57.466 : <epoch:  4, iter:   5,600, lr:1.000e-04> G_loss: 5.220e-02 
22-05-10 23:19:55.460 : <epoch:  5, iter:   5,800, lr:1.000e-04> G_loss: 6.315e-02 
22-05-10 23:22:51.619 : <epoch:  5, iter:   6,000, lr:1.000e-04> G_loss: 3.747e-02 
22-05-10 23:25:47.788 : <epoch:  5, iter:   6,200, lr:1.000e-04> G_loss: 6.518e-02 
22-05-10 23:28:44.226 : <epoch:  5, iter:   6,400, lr:1.000e-04> G_loss: 5.652e-02 
22-05-10 23:31:40.455 : <epoch:  5, iter:   6,600, lr:1.000e-04> G_loss: 6.511e-02 
22-05-10 23:34:36.324 : <epoch:  5, iter:   6,800, lr:1.000e-04> G_loss: 3.473e-02 
22-05-10 23:37:34.705 : <epoch:  6, iter:   7,000, lr:1.000e-04> G_loss: 3.092e-02 
22-05-10 23:40:30.875 : <epoch:  6, iter:   7,200, lr:1.000e-04> G_loss: 6.071e-02 
22-05-10 23:43:26.873 : <epoch:  6, iter:   7,400, lr:1.000e-04> G_loss: 3.609e-02 
22-05-10 23:46:23.143 : <epoch:  6, iter:   7,600, lr:1.000e-04> G_loss: 3.812e-02 
22-05-10 23:49:19.181 : <epoch:  6, iter:   7,800, lr:1.000e-04> G_loss: 6.638e-02 
22-05-10 23:52:14.848 : <epoch:  6, iter:   8,000, lr:1.000e-04> G_loss: 2.151e-02 
22-05-10 23:55:11.468 : <epoch:  7, iter:   8,200, lr:1.000e-04> G_loss: 3.049e-02 
22-05-10 23:58:06.372 : <epoch:  7, iter:   8,400, lr:1.000e-04> G_loss: 3.437e-02 
22-05-11 00:01:01.096 : <epoch:  7, iter:   8,600, lr:1.000e-04> G_loss: 4.790e-02 
22-05-11 00:03:55.753 : <epoch:  7, iter:   8,800, lr:1.000e-04> G_loss: 8.127e-02 
22-05-11 00:06:50.805 : <epoch:  7, iter:   9,000, lr:1.000e-04> G_loss: 3.658e-02 
22-05-11 00:09:45.561 : <epoch:  7, iter:   9,200, lr:1.000e-04> G_loss: 3.099e-02 
22-05-11 00:12:43.268 : <epoch:  8, iter:   9,400, lr:1.000e-04> G_loss: 3.936e-02 
22-05-11 00:15:38.961 : <epoch:  8, iter:   9,600, lr:1.000e-04> G_loss: 4.890e-02 
22-05-11 00:18:35.650 : <epoch:  8, iter:   9,800, lr:1.000e-04> G_loss: 4.451e-02 
22-05-11 00:21:31.398 : <epoch:  8, iter:  10,000, lr:1.000e-04> G_loss: 3.802e-02 
22-05-11 00:21:31.399 : Saving the model.
22-05-11 00:24:29.149 : <epoch:  8, iter:  10,200, lr:1.000e-04> G_loss: 3.826e-02 
22-05-11 00:27:27.027 : <epoch:  9, iter:  10,400, lr:1.000e-04> G_loss: 3.582e-02 
22-05-11 00:30:23.801 : <epoch:  9, iter:  10,600, lr:1.000e-04> G_loss: 4.155e-02 
22-05-11 00:33:20.874 : <epoch:  9, iter:  10,800, lr:1.000e-04> G_loss: 4.279e-02 
22-05-11 00:36:17.278 : <epoch:  9, iter:  11,000, lr:1.000e-04> G_loss: 3.458e-02 
22-05-11 00:39:13.255 : <epoch:  9, iter:  11,200, lr:1.000e-04> G_loss: 4.600e-02 
22-05-11 00:42:09.386 : <epoch:  9, iter:  11,400, lr:1.000e-04> G_loss: 3.282e-02 
22-05-11 00:45:08.013 : <epoch: 10, iter:  11,600, lr:1.000e-04> G_loss: 3.399e-02 
22-05-11 00:48:04.209 : <epoch: 10, iter:  11,800, lr:1.000e-04> G_loss: 4.459e-02 
22-05-11 00:51:00.789 : <epoch: 10, iter:  12,000, lr:1.000e-04> G_loss: 2.807e-02 
22-05-11 00:53:57.095 : <epoch: 10, iter:  12,200, lr:1.000e-04> G_loss: 4.666e-02 
22-05-11 00:56:54.931 : <epoch: 10, iter:  12,400, lr:1.000e-04> G_loss: 2.236e-02 
22-05-11 00:59:53.891 : <epoch: 10, iter:  12,600, lr:1.000e-04> G_loss: 4.646e-02 
22-05-11 01:02:55.498 : <epoch: 11, iter:  12,800, lr:1.000e-04> G_loss: 5.620e-02 
22-05-11 01:05:54.304 : <epoch: 11, iter:  13,000, lr:1.000e-04> G_loss: 2.653e-02 
22-05-11 01:08:53.578 : <epoch: 11, iter:  13,200, lr:1.000e-04> G_loss: 4.859e-02 
22-05-11 01:11:52.366 : <epoch: 11, iter:  13,400, lr:1.000e-04> G_loss: 7.580e-02 
22-05-11 01:14:50.976 : <epoch: 11, iter:  13,600, lr:1.000e-04> G_loss: 4.450e-02 
22-05-11 01:17:48.797 : <epoch: 11, iter:  13,800, lr:1.000e-04> G_loss: 5.251e-02 
22-05-11 01:20:48.002 : <epoch: 12, iter:  14,000, lr:1.000e-04> G_loss: 1.481e-02 
22-05-11 01:23:45.548 : <epoch: 12, iter:  14,200, lr:1.000e-04> G_loss: 2.222e-02 
22-05-11 01:26:44.149 : <epoch: 12, iter:  14,400, lr:1.000e-04> G_loss: 5.210e-02 
22-05-11 01:29:41.715 : <epoch: 12, iter:  14,600, lr:1.000e-04> G_loss: 5.818e-02 
22-05-11 01:32:39.450 : <epoch: 12, iter:  14,800, lr:1.000e-04> G_loss: 3.104e-02 
22-05-11 01:35:40.211 : <epoch: 13, iter:  15,000, lr:1.000e-04> G_loss: 1.961e-02 
22-05-11 01:35:40.213 : Saving the model.
22-05-11 01:38:40.747 : <epoch: 13, iter:  15,200, lr:1.000e-04> G_loss: 7.196e-02 
22-05-11 01:41:38.500 : <epoch: 13, iter:  15,400, lr:1.000e-04> G_loss: 6.465e-02 
22-05-11 01:44:35.521 : <epoch: 13, iter:  15,600, lr:1.000e-04> G_loss: 2.569e-02 
22-05-11 01:47:32.310 : <epoch: 13, iter:  15,800, lr:1.000e-04> G_loss: 5.204e-02 
22-05-11 01:50:29.462 : <epoch: 13, iter:  16,000, lr:1.000e-04> G_loss: 2.851e-02 
22-05-11 01:53:29.771 : <epoch: 14, iter:  16,200, lr:1.000e-04> G_loss: 4.482e-02 
22-05-11 01:56:26.900 : <epoch: 14, iter:  16,400, lr:1.000e-04> G_loss: 4.132e-02 
22-05-11 01:59:23.245 : <epoch: 14, iter:  16,600, lr:1.000e-04> G_loss: 2.570e-02 
22-05-11 02:02:20.553 : <epoch: 14, iter:  16,800, lr:1.000e-04> G_loss: 7.769e-02 
22-05-11 02:05:17.421 : <epoch: 14, iter:  17,000, lr:1.000e-04> G_loss: 2.359e-02 
22-05-11 02:08:14.091 : <epoch: 14, iter:  17,200, lr:1.000e-04> G_loss: 3.988e-02 
22-05-11 02:11:12.930 : <epoch: 15, iter:  17,400, lr:1.000e-04> G_loss: 3.519e-02 
22-05-11 02:14:09.940 : <epoch: 15, iter:  17,600, lr:1.000e-04> G_loss: 3.712e-02 
22-05-11 02:17:08.563 : <epoch: 15, iter:  17,800, lr:1.000e-04> G_loss: 3.568e-02 
22-05-11 02:20:07.143 : <epoch: 15, iter:  18,000, lr:1.000e-04> G_loss: 5.147e-02 
22-05-11 02:23:05.927 : <epoch: 15, iter:  18,200, lr:1.000e-04> G_loss: 4.345e-02 
22-05-11 02:26:03.890 : <epoch: 15, iter:  18,400, lr:1.000e-04> G_loss: 4.681e-02 
22-05-11 02:29:02.726 : <epoch: 16, iter:  18,600, lr:1.000e-04> G_loss: 4.955e-02 
22-05-11 02:32:00.830 : <epoch: 16, iter:  18,800, lr:1.000e-04> G_loss: 3.684e-02 
22-05-11 02:34:58.683 : <epoch: 16, iter:  19,000, lr:1.000e-04> G_loss: 1.822e-02 
22-05-11 02:37:56.260 : <epoch: 16, iter:  19,200, lr:1.000e-04> G_loss: 2.493e-02 
22-05-11 02:40:55.331 : <epoch: 16, iter:  19,400, lr:1.000e-04> G_loss: 4.806e-02 
22-05-11 02:43:55.802 : <epoch: 17, iter:  19,600, lr:1.000e-04> G_loss: 5.643e-02 
22-05-11 02:46:55.015 : <epoch: 17, iter:  19,800, lr:1.000e-04> G_loss: 4.946e-02 
22-05-11 02:49:53.439 : <epoch: 17, iter:  20,000, lr:1.000e-04> G_loss: 2.293e-02 
22-05-11 02:49:53.440 : Saving the model.
Traceback (most recent call last):
  File "main_train_psnr.py", line 261, in <module>
    main()
  File "main_train_psnr.py", line 229, in main
    model.test()
  File "/home/jiezcao/hongyu/USRSWINet/models/model_plain.py", line 186, in test
    self.netG_forward()
  File "/home/jiezcao/hongyu/USRSWINet/models/model_plain4.py", line 23, in netG_forward
    self.E = self.netG(self.L, self.k, self.sf, self.sigma)
  File "/scratch_net/ken/jiezcao/anaconda3/envs/usrswin/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/scratch_net/ken/jiezcao/anaconda3/envs/usrswin/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 165, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/scratch_net/ken/jiezcao/anaconda3/envs/usrswin/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/jiezcao/hongyu/USRSWINet/models/network_usrnet_ST.py", line 322, in forward
    x = self.p(torch.cat((x, ab[:, i+self.n:i+self.n+1, ...].repeat(1, 1, x.size(2), x.size(3))), dim=1))
  File "/scratch_net/ken/jiezcao/anaconda3/envs/usrswin/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/jiezcao/hongyu/USRSWINet/models/network_usrnet_ST.py", line 196, in forward
    x = self.m_up3(x+x4)
RuntimeError: The size of tensor a (42) must match the size of tensor b (41) at non-singleton dimension 2
